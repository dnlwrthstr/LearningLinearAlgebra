{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6e9c1fc072236f",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Orthogonal and orthonormal bases\n",
    "\n",
    "You already know that any two vectors that do not lie on the same line form a basis in the plane. However, if you ask a random person to draw two coordinate axes on a checkered piece of paper, they will most likely draw two **perpendicular** lines.\n",
    "\n",
    "This intuition reflects an important idea: the concept of **orthogonality** complements the notion of choosing a basis in a vector space in a very natural and powerful way.\n",
    "\n",
    "The combination of **orthogonality** and **bases** is extremely fruitful and appears throughout mathematics and its applications. It plays a fundamental role in areas such as:\n",
    "- dimensionality reduction in machine learning,\n",
    "- Fourier analysis,\n",
    "- signal processing,\n",
    "- and many frontiers of modern science and technology.\n",
    "\n",
    "The scope of applications of orthogonal bases is so vast that providing a complete overview would be difficult. Therefore, we will focus on understanding the **foundations** of these ideas and where they come from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601c22c8f83f7c9",
   "metadata": {},
   "source": [
    "## Example: Constructing an Orthogonal and Orthonormal Basis\n",
    "\n",
    "Consider a two-dimensional Euclidean space\n",
    "$$\n",
    "(V, \\langle \\cdot , \\cdot \\rangle).\n",
    "$$\n",
    "Choose arbitrary vectors $\\vec v_1$ and $\\vec v_2$ forming a basis of $V$.\n",
    "\n",
    "Define a new vector\n",
    "$$\n",
    "\\vec v_2' = \\vec v_2 - \\frac{\\langle \\vec v_1, \\vec v_2 \\rangle}{\\langle \\vec v_1, \\vec v_1 \\rangle}\\,\\vec v_1.\n",
    "$$\n",
    "\n",
    "This vector is of particular interest because it is **orthogonal** to $\\vec v_1$.\n",
    "\n",
    "### Orthogonality Check\n",
    "\n",
    "Indeed,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\langle \\vec v_1, \\vec v_2' \\rangle\n",
    "&= \\left\\langle \\vec v_1,\\; \\vec v_2 - \\frac{\\langle \\vec v_1, \\vec v_2 \\rangle}{\\langle \\vec v_1, \\vec v_1 \\rangle}\\vec v_1 \\right\\rangle \\\\\n",
    "&= \\langle \\vec v_1, \\vec v_2 \\rangle\n",
    "- \\frac{\\langle \\vec v_1, \\vec v_2 \\rangle}{\\langle \\vec v_1, \\vec v_1 \\rangle}\n",
    "\\langle \\vec v_1, \\vec v_1 \\rangle \\\\\n",
    "&= 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $\\{\\vec v_1, \\vec v_2\\}$ is a basis, vectors $\\vec v_1$ and $\\vec v_2'$ are linearly independent. Therefore,\n",
    "$$\n",
    "\\{\\vec v_1, \\vec v_2'\\}\n",
    "$$\n",
    "is also a basis of $V$, now consisting of **orthogonal vectors**.\n",
    "\n",
    "Such a basis is called an **orthogonal basis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfeb2643f5a7a77",
   "metadata": {},
   "source": [
    "\n",
    "### Numerical Example\n",
    "\n",
    "Suppose\n",
    "$$\n",
    "\\langle \\vec v_1, \\vec v_1 \\rangle = 9, \\quad\n",
    "\\langle \\vec v_2, \\vec v_2 \\rangle = 8, \\quad\n",
    "\\langle \\vec v_1, \\vec v_2 \\rangle = 6.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\vec v_2' = \\vec v_2 - \\frac{6}{9}\\vec v_1\n",
    "= \\vec v_2 - \\frac{2}{3}\\vec v_1.\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\{\\vec v_1,\\; \\vec v_2 - \\tfrac{2}{3}\\vec v_1\\}\n",
    "$$\n",
    "is an orthogonal basis.\n",
    "\n",
    "### Lengths of the Orthogonal Vectors\n",
    "\n",
    "The norms are:\n",
    "$$\n",
    "\\|\\vec v_1\\| = \\sqrt{\\langle \\vec v_1, \\vec v_1 \\rangle} = 3,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|\\vec v_2'\\|\n",
    "&= \\sqrt{\\langle \\vec v_2 - \\tfrac{2}{3}\\vec v_1,\\; \\vec v_2 - \\tfrac{2}{3}\\vec v_1 \\rangle} \\\\\n",
    "&= \\sqrt{\n",
    "\\langle \\vec v_2, \\vec v_2 \\rangle\n",
    "- \\tfrac{4}{3}\\langle \\vec v_1, \\vec v_2 \\rangle\n",
    "+ \\tfrac{4}{9}\\langle \\vec v_1, \\vec v_1 \\rangle\n",
    "} \\\\\n",
    "&= \\sqrt{8 - 8 + 4} = 2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Dividing a vector by its norm produces a **unit vector** (length $1$). This process is called **normalization**.\n",
    "\n",
    "For any vector $\\vec w \\neq 0$,\n",
    "$$\n",
    "\\left\\|\\frac{\\vec w}{\\|\\vec w\\|}\\right\\| = 1.\n",
    "$$\n",
    "\n",
    "Normalize $\\vec v_1$ and $\\vec v_2'$:\n",
    "$$\n",
    "\\vec e_1 = \\frac{\\vec v_1}{\\|\\vec v_1\\|} = \\frac{1}{3}\\vec v_1,\n",
    "$$\n",
    "$$\n",
    "\\vec e_2 = \\frac{\\vec v_2'}{\\|\\vec v_2'\\|}\n",
    "= \\frac{1}{2}\\vec v_2'\n",
    "= \\frac{1}{2}\\vec v_2 - \\frac{1}{3}\\vec v_1.\n",
    "$$\n",
    "\n",
    "### Result: Orthonormal Basis\n",
    "\n",
    "The vectors $\\vec e_1$ and $\\vec e_2$ form a basis of $V$ that is both **orthogonal** and **normalized**:\n",
    "\n",
    "$$\n",
    "\\langle \\vec e_1, \\vec e_1 \\rangle = 1, \\quad\n",
    "\\langle \\vec e_2, \\vec e_2 \\rangle = 1, \\quad\n",
    "\\langle \\vec e_1, \\vec e_2 \\rangle = 0.\n",
    "$$\n",
    "\n",
    "These relations are exactly the same as those of the standard basis in $\\mathbb{R}^2$ with the usual dot product.\n",
    "\n",
    "This procedure is the two-dimensional case of the **Gram–Schmidt orthonormalization process**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ce2816b0ace75",
   "metadata": {},
   "source": [
    "## Geometric Side of the Story\n",
    "\n",
    "All the algebraic manipulations introduced earlier may seem somewhat overcomplicated at first. However, when viewed from a **geometric perspective**, they become much more intuitive and natural.\n",
    "\n",
    "Let us once again consider two vectors\n",
    "$$\n",
    "\\vec v_1 \\quad \\text{and} \\quad \\vec v_2,\n",
    "$$\n",
    "but now interpret them as **arrows in the plane**.\n",
    "\n",
    "![Vectors v1 and v2](img/v1_v2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d25c60a20ae3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T19:53:05.391836Z",
     "start_time": "2026-01-18T19:53:05.375091Z"
    }
   },
   "source": [
    "Now notice that the vector\n",
    "\n",
    "$$\n",
    "\\frac{\\langle \\vec v_1, \\vec v_2 \\rangle}{\\langle \\vec v_1, \\vec v_1 \\rangle}\n",
    "\\, \\vec v_1\n",
    "$$\n",
    "\n",
    "is just the projection\n",
    "\n",
    "$$\n",
    "\\operatorname{proj}_{\\vec v_1}(\\vec v_2)\n",
    "$$\n",
    "\n",
    "of the vector $\\vec v_2$ onto $\\vec v_1$.\n",
    "\n",
    "This projection can be easily illustrated using the following picture:\n",
    "\n",
    "![Projection of v2 on v1](img/proj_v2_on_v1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5959f386351e22",
   "metadata": {},
   "source": [
    "Therefore, the vector $\\vec v_2'$ is the vector $\\vec v_2$ from which you have “removed” its projection onto $\\vec v_1$:\n",
    "\n",
    "$$\n",
    "\\vec v_2'\n",
    "=\n",
    "\\vec v_2\n",
    "-\n",
    "\\operatorname{proj}_{\\vec v_1}(\\vec v_2).\n",
    "$$\n",
    "\n",
    "You have already proven that $\\vec v_2'$ is perpendicular to $\\vec v_1$.\n",
    "However, now it is also easy to see this:\n",
    "\n",
    "![Projection Components](img/proj_components.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3bad40265d78d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T19:53:05.411422Z",
     "start_time": "2026-01-18T19:53:05.395620Z"
    }
   },
   "source": [
    "Last but not least, you normalize the vectors so that their lengths are equal to $1$.\n",
    "In our picture, it looks like this.\n",
    "\n",
    "![Projection Basis](img/proj_basis.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646d580c59f517c",
   "metadata": {},
   "source": [
    "Here $\\vec e_1$ and $\\vec e_2$ are normalized versions of $\\vec v_1$ and $\\vec v_2'$, respectively.\n",
    "They form a basis of this plane, which is very similar to the one you usually choose in a checkered notebook.\n",
    "\n",
    "![Grid Basis](img/grid_basis.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115c117ae39d8b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T19:53:05.422559Z",
     "start_time": "2026-01-18T19:53:05.412275Z"
    }
   },
   "source": [
    "## Higher dimensions\n",
    "\n",
    "Now this idea of basis with length-one vectors, which are orthogonal to each other, could be adapted to an arbitrary dimension $n$.\n",
    "First, let’s give it a name.\n",
    "\n",
    "Let $(V,\\langle \\cdot,\\cdot\\rangle)$ be a Euclidean space $(\\dim(V)=n)$.\n",
    "A basis\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "is called **orthogonal** if\n",
    "$$\n",
    "\\langle \\vec e_i, \\vec e_j \\rangle = 0\n",
    "$$\n",
    "for any $i,j \\in \\{1,2,\\ldots,n\\}$ such that $i \\neq j$.\n",
    "It literally means that each vector of this basis is **orthogonal** to any other vector.\n",
    "\n",
    "An orthonormal basis\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "is called **orthonormal** if\n",
    "$$\n",
    "\\langle \\vec e_i, \\vec e_i \\rangle = 1\n",
    "$$\n",
    "for any $i \\in \\{1,2,\\ldots,n\\}$.\n",
    "This means that besides orthogonality, each vector is a unit vector.\n",
    "\n",
    "There is a very useful conventional symbol in mathematics, which is called the **Kronecker delta**.\n",
    "It is defined in the following manner:\n",
    "$$\n",
    "\\delta_{i,j} =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } i=j, \\\\\n",
    "0, & \\text{if } i\\neq j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "It means that $\\delta_{4,4}$ and $\\delta_{3,5}$ mean the same as $1$ and $0$ correspondingly.\n",
    "How could it be used?\n",
    "Well, you can say that a basis\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "is orthonormal if\n",
    "$$\n",
    "\\langle \\vec e_i, \\vec e_j \\rangle = \\delta_{i,j}\n",
    "$$\n",
    "for $i,j \\in \\{1,2,\\ldots,n\\}$.\n",
    "This way, you can combine the two above-mentioned definitions into one.\n",
    "\n",
    "You will learn more on why orthonormal bases are so great later.\n",
    "But first, let's state that such a basis could be constructed for any finite-dimensional vector space.\n",
    "The process of this construction is called the **Gram–Schmidt process**, and it goes like this:\n",
    "\n",
    "Let’s start with $V$ (with inner product $\\langle \\cdot,\\cdot\\rangle$ and $\\dim(V)=n$) and its arbitrary basis $\\{\\vec v_1, \\vec v_2, \\ldots, \\vec v_n\\}.$\n",
    "\n",
    "Introduce a new basis $\\{\\vec w_1, \\vec w_2, \\ldots, \\vec w_n\\}$ in the following manner.\n",
    "\n",
    "1. Vector $\\vec w_1$ is equal to $\\vec v_1$.\n",
    "\n",
    "2. Each following vector $\\vec w_k$ (for $1 < k \\le n$) is defined as the vector $\\vec v_k$\n",
    "from which you have “removed” all the projections onto the previous vectors\n",
    "$\\vec v_1, \\vec v_2, \\ldots, \\vec v_{k-1}$:\n",
    "$$\n",
    "\\vec w_k\n",
    "=\n",
    "\\vec v_k\n",
    "-\n",
    "\\operatorname{proj}_{\\vec w_1}(\\vec v_k)\n",
    "-\n",
    "\\operatorname{proj}_{\\vec w_2}(\\vec v_k)\n",
    "-\n",
    "\\cdots\n",
    "-\n",
    "\\operatorname{proj}_{\\vec w_{k-1}}(\\vec v_k).\n",
    "$$\n",
    "These vectors turn out to be orthogonal to each other and form a basis of $V$.\n",
    "\n",
    "3. Finally, normalize vectors $\\vec w_k$:\n",
    "$$\\vec e_k = \\frac{1}{\\|\\vec w_k\\|}\\,\\vec w_k.$$\n",
    "Vectors $\\vec e_k$ are still orthogonal, but also have length $1$.\n",
    "Therefore, $\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}$ is an orthonormal basis of $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb9f9a8438c486",
   "metadata": {},
   "source": [
    "The proof of orthogonality of $\\{\\vec w_1, \\vec w_2, \\ldots, \\vec w_n\\}$ is actually quite a tricky task. Think about it like this: by ‘removing’ all the components of the vector that are co-directed with\n",
    "$\\vec v_1, \\vec v_2, \\ldots, \\vec v_{k-1}$, you end up with a vector that is perpendicular to all of them.\n",
    "\n",
    "Here’s an example for a better understanding.\n",
    "Consider three linearly independent vectors in a Euclidean space $\\mathbb{R}^3$\n",
    "with the choice of dot product as an inner product (meaning that\n",
    "$\\langle \\vec v, \\vec w \\rangle = \\vec v \\cdot \\vec w$):\n",
    "\n",
    "$$\n",
    "\\vec v_1 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ 0 \\\\ 1\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\vec v_2 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ -2 \\\\ 0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\vec v_3 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ -1 \\\\ 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Applying the Gram–Schmidt process:\n",
    "\n",
    "$$\n",
    "\\vec w_1 = \\vec v_1 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ 0 \\\\ 1\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\vec e_1 =\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec w_2\n",
    "=\n",
    "\\vec v_2\n",
    "-\n",
    "\\frac{\\langle \\vec w_1, \\vec v_2 \\rangle}\n",
    "{\\langle \\vec w_1, \\vec w_1 \\rangle}\n",
    "\\, \\vec w_1\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{2} \\\\ -2 \\\\ -\\frac{1}{2}\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\vec e_2 =\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{6}} \\\\ -\\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec w_3\n",
    "=\n",
    "\\vec v_3\n",
    "-\n",
    "\\frac{\\langle \\vec w_2, \\vec v_3 \\rangle}\n",
    "{\\langle \\vec w_2, \\vec w_2 \\rangle}\n",
    "\\, \\vec w_2\n",
    "-\n",
    "\\frac{\\langle \\vec w_1, \\vec v_3 \\rangle}\n",
    "{\\langle \\vec w_1, \\vec w_1 \\rangle}\n",
    "\\, \\vec w_1\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "-\\frac{2}{9} \\\\ -\\frac{1}{9} \\\\ \\frac{2}{9}\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\vec e_3 =\n",
    "\\begin{pmatrix}\n",
    "-\\frac{2}{3} \\\\ -\\frac{1}{3} \\\\ \\frac{2}{3}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "By calculating the dot products, you can check that\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\vec e_3\\}\n",
    "$$\n",
    "is indeed an orthonormal basis of $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3370076e8e0b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T19:53:05.434291Z",
     "start_time": "2026-01-18T19:53:05.425706Z"
    }
   },
   "source": [
    "## Features of an orthonormal basis\n",
    "\n",
    "The main feature of choice of an orthonormal basis in $V$ is that it turns $V$ into $\\mathbb{R}^n$\n",
    "(here $n=\\dim(V)$) and $\\langle \\cdot,\\cdot\\rangle$ into a standard dot product.\n",
    "Let see how it works.\n",
    "\n",
    "If\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "is an arbitrary basis of $V$ then you can express the inner product of vectors\n",
    "$$\n",
    "\\vec a\n",
    "=\n",
    "a_1 \\vec e_1 + a_2 \\vec e_2 + \\cdots + a_n \\vec e_n\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\vec b\n",
    "=\n",
    "b_1 \\vec e_1 + b_2 \\vec e_2 + \\cdots + b_n \\vec e_n\n",
    "$$\n",
    "only with the help of all possible products of the form\n",
    "$\\langle \\vec e_i, \\vec e_j \\rangle$ where $i \\le j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\langle \\vec a, \\vec b \\rangle\n",
    "&=\n",
    "a_1 b_1 \\langle \\vec e_1, \\vec e_1 \\rangle\n",
    "+\n",
    "a_2 b_2 \\langle \\vec e_2, \\vec e_2 \\rangle\n",
    "+\n",
    "\\cdots\n",
    "+\n",
    "a_n b_n \\langle \\vec e_n, \\vec e_n \\rangle\n",
    "\\\\\n",
    "&\\quad+\n",
    "(a_1 b_2 + a_2 b_1)\\langle \\vec e_1, \\vec e_2 \\rangle\n",
    "+\n",
    "(a_1 b_3 + a_3 b_1)\\langle \\vec e_1, \\vec e_3 \\rangle\n",
    "+\n",
    "\\cdots\n",
    "+\n",
    "(a_{n-1} b_n + a_n b_{n-1})\\langle \\vec e_{n-1}, \\vec e_n \\rangle.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is one monstrous expression.\n",
    "The problem with it is that starting with $n=4$ the number of terms with $i<j$\n",
    "is greater than with $i=j$ and it grows in a quadratic manner with the increase of $n$.\n",
    "That means that computations of inner products in an arbitrary basis are much harder than in\n",
    "$\\mathbb{R}^n$ with standard dot product, in which the number of terms is just $n$.\n",
    "\n",
    "But notice that if\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "is an orthonormal basis, this problem vanishes.\n",
    "Why?\n",
    "Because all the terms with $i<j$ in this enormous formula will be equal to $0$.\n",
    "Moreover, all the other terms will be equal to $1$.\n",
    "Which results in a very elegant formula\n",
    "\n",
    "$$\n",
    "\\langle \\vec a, \\vec b \\rangle\n",
    "=\n",
    "a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n.\n",
    "$$\n",
    "\n",
    "Not only this formula is way easier, but it is also very familiar.\n",
    "It is just a dot product of vectors\n",
    "$$\n",
    "(a_1, a_2, \\ldots, a_n)^T \\in \\mathbb{R}^n\n",
    "$$\n",
    "and\n",
    "$$\n",
    "(b_1, b_2, \\ldots, b_n)^T \\in \\mathbb{R}^n.\n",
    "$$\n",
    "\n",
    "Take into account that writing down\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "a_1+b_1 \\\\ a_2+b_2 \\\\ \\vdots \\\\ a_n+b_n\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\lambda\n",
    "\\begin{pmatrix}\n",
    "a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\lambda a_1 \\\\ \\lambda a_2 \\\\ \\vdots \\\\ \\lambda a_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is just another way of writing\n",
    "\n",
    "$$\n",
    "(a_1 \\vec e_1 + a_2 \\vec e_2 + \\cdots + a_n \\vec e_n)\n",
    "+\n",
    "(b_1 \\vec e_1 + b_2 \\vec e_2 + \\cdots + b_n \\vec e_n)\n",
    "=\n",
    "((a_1+b_1)\\vec e_1 + (a_2+b_2)\\vec e_2 + \\cdots + (a_n+b_n)\\vec e_n),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda (a_1 \\vec e_1 + a_2 \\vec e_2 + \\cdots + a_n \\vec e_n)\n",
    "=\n",
    "\\lambda a_1 \\vec e_1 + \\lambda a_2 \\vec e_2 + \\cdots + \\lambda a_n \\vec e_n.\n",
    "$$\n",
    "\n",
    "You can conclude that writing down vectors in an orthogonal basis of a Euclidean space $V$\n",
    "is basically the same as working with $\\mathbb{R}^n$ with a standard dot product.\n",
    "\n",
    "There is one more way to express this property.\n",
    "Let $\\vec x$ be a vector in a Euclidean space $(V,\\langle \\cdot,\\cdot\\rangle)$.\n",
    "Let\n",
    "$$\n",
    "\\{\\vec e_1, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "be an orthonormal basis of this space and\n",
    "$$\n",
    "\\vec x = x_1 \\vec e_1 + \\cdots + x_n \\vec e_n.\n",
    "$$\n",
    "\n",
    "Now let’s calculate the following sum\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\langle \\vec x, \\vec e_i \\rangle \\cdot \\vec e_i\n",
    "=\n",
    "\\langle \\vec x, \\vec e_1 \\rangle \\cdot \\vec e_1\n",
    "+\n",
    "\\cdots\n",
    "+\n",
    "\\langle \\vec x, \\vec e_n \\rangle \\cdot \\vec e_n.\n",
    "$$\n",
    "\n",
    "If you throw out all the inner products that are equal to $0$ you will end up with\n",
    "\n",
    "$$\n",
    "\\langle \\vec x, \\vec e_1 \\rangle \\cdot \\vec e_1\n",
    "+\n",
    "\\cdots\n",
    "+\n",
    "\\langle \\vec x, \\vec e_n \\rangle \\cdot \\vec e_n\n",
    "=\n",
    "x_1 \\vec e_1 + \\cdots + x_n \\vec e_n.\n",
    "$$\n",
    "\n",
    "This literally means that\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\langle \\vec x, \\vec e_i \\rangle \\cdot \\vec e_i = \\vec x.\n",
    "$$\n",
    "\n",
    "Therefore the coordinates $x_i$ are just the projections of $\\vec x$ onto $\\vec e_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e13dafe0625e81",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Let $(V,\\langle \\cdot,\\cdot\\rangle)$ be a Euclidean space and $\\dim(V)=n$.\n",
    "\n",
    "A normalization of a vector $\\vec v$ is a unit vector\n",
    "$$\n",
    "\\frac{1}{\\|\\vec v\\|}\\cdot \\vec v.\n",
    "$$\n",
    "\n",
    "A basis\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "is orthogonal if\n",
    "$$\n",
    "\\langle \\vec e_i, \\vec e_j \\rangle = 0\n",
    "$$\n",
    "for all $i \\ne j$.\n",
    "\n",
    "A basis\n",
    "$$\n",
    "\\{\\vec e_1, \\vec e_2, \\ldots, \\vec e_n\\}\n",
    "$$\n",
    "is orthonormal if\n",
    "$$\n",
    "\\langle \\vec e_i, \\vec e_j \\rangle = \\delta_{i,j}.\n",
    "$$\n",
    "\n",
    "If you write the vectors of $V$ in an orthogonal basis, then $V$ could be thought as $\\mathbb{R}^n$ and $\\langle \\cdot,\\cdot\\rangle$ as the dot product.\n",
    "\n",
    "Knowing some base of a Euclidean space you can always obtain an orthonormal basis of this space by the Dram-Schmidt process.\n",
    "\n",
    "The following identity holds:\n",
    "$$\n",
    "\\vec x\n",
    "=\n",
    "\\langle \\vec x, \\vec e_1 \\rangle \\cdot \\vec e_1\n",
    "+\n",
    "\\ldots\n",
    "+\n",
    "\\langle \\vec x, \\vec e_n \\rangle \\cdot \\vec e_n.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
