{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f117667016c2c055",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577be6a2af8eeef",
   "metadata": {},
   "source": [
    "This is the deepest, most universal, and most widely used theorem in all linear algebra applications. Absolutely any matrix admits this decomposition. Once you calculate it, you have access to almost all of its properties, from its range to its inverse, through its determinant and a complete interpretation of its associated linear transformation.\n",
    "\n",
    "In a sense, this is why you've been learning linear algebra. In this introductory topic, you'll analyze this decomposition in detail, learn how to calculate it efficiently, and finally take a look at the first sample of its geometric implications. In the next topic, you'll see its power in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4473aea25ea63e",
   "metadata": {},
   "source": [
    "## The best decomposition\n",
    "\n",
    "For any $m\\times n$ matrix $A$, there exist orthogonal matrices $U$ and $V$ (of sizes $m$ and $n$, respectively), and a diagonal $m\\times n$ matrix $\\Sigma$ with non-negative entries, such that\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9cb72d47cf366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T17:57:46.647015Z",
     "start_time": "2026-01-09T17:57:46.591600Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('img/singular_value_decomposition.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37436f84026ea3",
   "metadata": {},
   "source": [
    "The fact that the rectangular matrix $\\Sigma$ is meant to be diagonal simply means that its $(i,j)$-entry is $0$ whenever $i\\ne j$. This decomposition brings with it a bit of terminology that's worth knowing now.\n",
    "\n",
    "| Concept | Name | Definition |\n",
    "|--------|------|------------|\n",
    "| The diagonal entries of $\\Sigma$ | Singular values of $A$ | The non-negative square roots of the eigenvalues of $AA^T$ |\n",
    "| The columns of $U$ | Left singular vectors of $A$ | The eigenvectors of $AA^T$ |\n",
    "| The columns of $V$ | Right singular vectors of $A$ | The eigenvectors of $A^TA$ |\n",
    "\n",
    "---\n",
    "\n",
    "## Proof of the singular value decomposition\n",
    "\n",
    "You can suppose that $m\\ge n$ and work with $A^TA$ — otherwise, you only need to use $AA^T$. Well, as you already know, the $n\\times n$ matrix $A^TA$ is positive semidefinite. Thus it has a spectral decomposition\n",
    "\n",
    "$$\n",
    "A^TA = VDV^T\n",
    "$$\n",
    "\n",
    "with $n$ non-negative eigenvalues $d_1,d_2,\\dots,d_n$ in the diagonal of $D$.\n",
    "\n",
    "Thanks to this fact, you can build the $m\\times n$ matrix $\\Sigma$ by putting its $(j,j)$-entry as $\\sqrt{d_j}$ and as $0$ otherwise. Of course, $\\Sigma$ is the principal square root of $D$ but with extra zero rows so that it has the same number of rows as $A$. Then\n",
    "\n",
    "$$\n",
    "\\Sigma^T\\Sigma = D\n",
    "$$\n",
    "\n",
    "and this implies that\n",
    "\n",
    "$$\n",
    "A^TA\n",
    "= VDV^T\n",
    "= V\\Sigma^T\\Sigma V^T\n",
    "= (\\Sigma V^T)^T \\Sigma V^T.\n",
    "$$\n",
    "\n",
    "Take a closer look at the last equation. You have just proved that\n",
    "\n",
    "$$\n",
    "A^TA = (\\Sigma V)^T \\Sigma V^T.\n",
    "$$\n",
    "\n",
    "This is the orthogonal freedom, so there exists an orthogonal matrix $U$ such that\n",
    "\n",
    "$$\n",
    "A = U\\Sigma V^T.\n",
    "$$\n",
    "\n",
    "Yes, you have built the decomposition.\n",
    "\n",
    "But you aren't done yet. You have to justify the terminology of the pieces. When the decomposition is ready, it's clear that\n",
    "\n",
    "$$\n",
    "A^TA\n",
    "= (U\\Sigma V^T)^T U\\Sigma V^T\n",
    "= V(\\Sigma^T\\Sigma)V^T.\n",
    "$$\n",
    "\n",
    "Since this is a diagonalization of $A^TA$, it follows that the columns of $V$ are eigenvectors of $A^TA$ and the entries in the main diagonal of $\\Sigma^T\\Sigma$ are the corresponding eigenvalues. You can easily verify the corresponding statements about $U$ and $AA^T$ in a very similar way.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical computation\n",
    "\n",
    "Actually, these definitions give you a rough method to calculate the decomposition:\n",
    "\n",
    "1. Compute $AA^T$ and $A^TA$.\n",
    "2. Find the eigenvalues of both $AA^T$ and $A^TA$ (they share the same eigenvalues).\n",
    "3. Calculate the non-negative square roots of these eigenvalues and place them on the diagonal of $\\Sigma$.\n",
    "4. Compute the eigenvectors of $AA^T$, normalize them to have length $1$, and place them as the columns of $U$.\n",
    "5. Do the same for $A^TA$ to build the matrix $V$.\n",
    "6. Finally,\n",
    "   $$\n",
    "   A = U\\Sigma V^T.\n",
    "   $$\n",
    "\n",
    "In the following section, you'll develop a much shorter algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae77f65d13bc08a",
   "metadata": {},
   "source": [
    "## Building the decomposition\n",
    "\n",
    "The method from the previous section has a serious drawback: you have to build two matrices, $AA^T$ and $A^TA$, and then build a spectral decomposition for each one. This sounds like a lot of work. Let's take a closer look at the pieces of the decomposition in order to improve the algorithm.\n",
    "\n",
    "It's customary to denote the singular values as $\\sigma_1,\\sigma_2,\\dots,\\sigma_n$ and to order them in non-increasing order\n",
    "\n",
    "$$\n",
    "\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_{\\min(m,n)}.\n",
    "$$\n",
    "\n",
    "Since $\\Sigma$ is diagonal, its rank is the number of nonzero rows it has, which is therefore the number of non-zero singular values of $A$. If $r$ is the rank of $A$, then\n",
    "\n",
    "$$\n",
    "r=\\operatorname{rank}(A)=\\operatorname{rank}(U\\Sigma V^T)=\\operatorname{rank}(\\Sigma),\n",
    "$$\n",
    "\n",
    "thus:\n",
    "\n",
    "**The number of non-zero singular values of $A$ is exactly the rank $r$ of $A$.**\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r > 0,\n",
    "\\qquad\n",
    "\\sigma_{r+1}=\\cdots=\\sigma_{\\min(m,n)}=0.\n",
    "$$\n",
    "\n",
    "Let's analyze the pieces by expressing $U$ and $V$ in terms of their columns:\n",
    "\n",
    "$$\n",
    "U=[\\,u_1\\;u_2\\;\\cdots\\;u_m\\,],\n",
    "\\qquad\n",
    "V=[\\,v_1\\;v_2\\;\\cdots\\;v_n\\,].\n",
    "$$\n",
    "\n",
    "The matrix equation\n",
    "\n",
    "$$\n",
    "A=U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "implies that\n",
    "\n",
    "$$\n",
    "AV=U\\Sigma.\n",
    "$$\n",
    "\n",
    "Take a closer look at the columns of these new matrices:\n",
    "\n",
    "$$\n",
    "AV=[\\,Av_1\\;Av_2\\;\\cdots\\;Av_n\\,],\n",
    "$$\n",
    "\n",
    "$$\n",
    "U\\Sigma=[\\,\\sigma_1u_1\\;\\sigma_2u_2\\;\\cdots\\;\\sigma_ru_r\\;0\\;\\cdots\\;0\\,].\n",
    "$$\n",
    "\n",
    "Since these two matrices are equal, their columns are equal too. This implies that the first $r$ columns of $U$ can be obtained by\n",
    "\n",
    "$$\n",
    "u_j=\\frac{1}{\\sigma_j}Av_j\n",
    "\\qquad \\text{for all } j\\in\\{1,2,\\dots,r\\}.\n",
    "$$\n",
    "\n",
    "Then the rest of the columns of $U$ can be calculated by extending the available ones so that they form an orthonormal basis. So, you have developed an easier method to get the decomposition:\n",
    "\n",
    "1. Start by computing $A^TA$.\n",
    "2. Build a spectral decomposition for this matrix:\n",
    "   $$\n",
    "   A^TA = V(\\Sigma^T\\Sigma)V^T\n",
    "   $$\n",
    "   by ordering the positive singular values in decreasing order.\n",
    "3. Compute\n",
    "   $$\n",
    "   u_j=\\frac{1}{\\sigma_j}Av_j\n",
    "   \\qquad \\text{for every } j\\in\\{1,2,\\dots,r\\}.\n",
    "   $$\n",
    "4. Extend the set $\\{u_1,u_2,\\dots,u_r\\}$ to an orthonormal basis and put the vectors as the columns of $U$.\n",
    "\n",
    "Now, you should get your hands dirty with some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988843bf4cdf580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T17:56:06.018775Z",
     "start_time": "2026-01-09T17:56:06.015275Z"
    }
   },
   "source": [
    "## A direct example\n",
    "\n",
    "Consider the following rectangular matrix:\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "-1 & 1\\\\\n",
    "-1 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The first step is computing a spectral decomposition for the positive definite matrix:\n",
    "\n",
    "$$\n",
    "A^TA=\n",
    "\\begin{pmatrix}\n",
    "2 & 0 & -2\\\\\n",
    "0 & 2 & 0\\\\\n",
    "-2 & 0 & 2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Its eigenvalues are $4$, $2$, and $0$. This means the singular values of $A$ are their square roots, so\n",
    "\n",
    "$$\n",
    "\\sigma_1=\\sqrt{4}=2,\n",
    "\\qquad\n",
    "\\sigma_2=\\sqrt{2}.\n",
    "$$\n",
    "\n",
    "This also implies that\n",
    "\n",
    "$$\n",
    "\\operatorname{rank}(A)=2\n",
    "$$\n",
    "\n",
    "since there are only two positive singular values. Now, an orthonormal basis corresponding to the eigenvalues is:\n",
    "\n",
    "$$\n",
    "v_1=\\frac12\n",
    "\\begin{pmatrix}\n",
    "-1\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "v_2=\\frac12\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "2\\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "v_3=\\frac12\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Now put these vectors as the columns of the matrix $V$ and the singular values on the diagonal of the matrix $\\Sigma$:\n",
    "\n",
    "$$\n",
    "\\Sigma=\n",
    "\\begin{pmatrix}\n",
    "2 & 0 & 0\\\\\n",
    "0 & 2 & 0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "V=\\frac12\n",
    "\\begin{pmatrix}\n",
    "-1 & 0 & 1\\\\\n",
    "0 & 2 & 0\\\\\n",
    "1 & 0 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "After that, the two columns of $U$ can be obtained via\n",
    "\n",
    "$$\n",
    "u_1=\\frac{1}{\\sigma_1}Av_1\n",
    "=\\frac12\n",
    "\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "-1 & 1\\\\\n",
    "-1 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "-1/2\\\\\n",
    "0\\\\\n",
    "1/2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\frac12\n",
    "\\begin{pmatrix}\n",
    "-1\\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "u_2=\\frac{1}{\\sigma_2}Av_2\n",
    "=\\frac12\n",
    "\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "-1 & 1\\\\\n",
    "-1 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\frac12\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "U=\\frac12\n",
    "\\begin{pmatrix}\n",
    "-1 & 1\\\\\n",
    "1 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "And that's it. The SVD decomposition of $A$ is complete. You should verify that\n",
    "\n",
    "$$\n",
    "A=U\\Sigma V^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6eda2299b6c39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T17:57:46.651024Z",
     "start_time": "2026-01-09T17:57:46.647760Z"
    }
   },
   "source": [
    "## More steps\n",
    "\n",
    "Let's see a more difficult decomposition. Take a square matrix:\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "2 & 2 & -2\\\\\n",
    "-4 & -1 & 4\\\\\n",
    "-4 & 2 & 4\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Start by computing the positive definite matrix:\n",
    "\n",
    "$$\n",
    "A^TA=\n",
    "\\begin{pmatrix}\n",
    "36 & 0 & -36\\\\\n",
    "0 & 9 & 0\\\\\n",
    "-36 & 0 & 36\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Since its eigenvalues are $72$, $9$, and $0$, the singular values of $A$ are their square roots\n",
    "\n",
    "$$\n",
    "\\sigma_1=\\sqrt{72}=6\\sqrt{2},\n",
    "\\qquad\n",
    "\\sigma_2=\\sqrt{9}=3.\n",
    "$$\n",
    "\n",
    "An orthonormal basis corresponding to these eigenvalues is:\n",
    "\n",
    "$$\n",
    "v_1=\\frac12\n",
    "\\begin{pmatrix}\n",
    "-1\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "v_2=\\frac12\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "2\\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "v_3=\\frac12\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Now you can build matrices $\\Sigma$ and $V$:\n",
    "\n",
    "$$\n",
    "\\Sigma=\n",
    "\\begin{pmatrix}\n",
    "6\\sqrt{2} & 0 & 0\\\\\n",
    "0 & 3 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "V=\\frac12\n",
    "\\begin{pmatrix}\n",
    "-1 & 0 & 1\\\\\n",
    "0 & 2 & 0\\\\\n",
    "1 & 0 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "But $A$ only has two singular values, so you can only compute the first two columns of $U$:\n",
    "\n",
    "$$\n",
    "u_1=\\frac{1}{\\sigma_1}Av_1\n",
    "=\n",
    "\\frac{1}{6\\sqrt{2}}\n",
    "\\begin{pmatrix}\n",
    "2 & 2 & -2\\\\\n",
    "-4 & -1 & 4\\\\\n",
    "-4 & 2 & 4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "-1/2\\\\\n",
    "0\\\\\n",
    "1/2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\frac13\n",
    "\\begin{pmatrix}\n",
    "-1\\\\\n",
    "2\\\\\n",
    "2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_2=\\frac{1}{\\sigma_2}Av_2\n",
    "=\n",
    "\\frac13\n",
    "\\begin{pmatrix}\n",
    "2 & 2 & -2\\\\\n",
    "-4 & -1 & 4\\\\\n",
    "-4 & 2 & 4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\frac13\n",
    "\\begin{pmatrix}\n",
    "2\\\\\n",
    "-1\\\\\n",
    "2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "You can find the last column of $U$ by extending $\\{u_1,u_2\\}$ to an orthonormal basis of $\\mathbb{R}^3$, perhaps with the Gram–Schmidt process or with the cross product. This can lead you to the vector\n",
    "\n",
    "$$\n",
    "u_3=\\frac13(-2,-2,1)^T.\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "U=\\frac13\n",
    "\\begin{pmatrix}\n",
    "-1 & 2 & -2\\\\\n",
    "2 & -1 & -2\\\\\n",
    "2 & 2 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Again, try to check that\n",
    "\n",
    "$$\n",
    "A=U\\Sigma V^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc5b9b37821ec7",
   "metadata": {},
   "source": [
    "## Geometry\n",
    "\n",
    "You can easily decompose the geometric behavior of any matrix $A$ in simple steps. First, notice that the linear operator of $A$ is given by the following composition of simpler pieces:\n",
    "\n",
    "$$\n",
    "L_A = L_U \\circ L_D \\circ L_{V^T}.\n",
    "$$\n",
    "\n",
    "So, the behavior of $L_A$ is described by three stages:\n",
    "\n",
    "- As $V^T$ is orthogonal, the action of $L_{V^T}$ is just a rotation and/or reflection of $\\mathbb{R}^m$.\n",
    "- After that, the fact that $D$ is diagonal implies that $L_D$ only stretches and then embeds $\\mathbb{R}^m$ in $\\mathbb{R}^n$.\n",
    "- Finally, $L_U$ is a rotation and/or reflection of $\\mathbb{R}^n$.\n",
    "\n",
    "Furthermore, the two radii of the ellipse are exactly the singular values $\\sigma_1$ and $\\sigma_2$ of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa9c48e66d5898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T18:04:15.854520Z",
     "start_time": "2026-01-09T18:04:15.789429Z"
    }
   },
   "outputs": [],
   "source": [
    "img_geometric = Image.open('img/geometric_view_svd.png')\n",
    "plt.imshow(img_geometric)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eff2e79f789251",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Take an $m\\times n$ matrix $A$ with rank equal to $r$.\n",
    "\n",
    "There are orthogonal matrices $U$ and $V$ (of sizes $m$ and $n$, respectively), and a diagonal $m\\times n$ matrix $\\Sigma$ with non-negative entries, such that\n",
    "\n",
    "$$\n",
    "A = U\\Sigma V^T.\n",
    "$$\n",
    "\n",
    "The diagonal entries of $\\Sigma$ are the singular values of $A$.\n",
    "\n",
    "There are exactly $r$ non-zero singular values, which are in decreasing order\n",
    "\n",
    "$$\n",
    "\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r > 0.\n",
    "$$\n",
    "\n",
    "In order to compute the decomposition, you first calculate a spectral decomposition for\n",
    "\n",
    "$$\n",
    "A^TA = VDV^T\n",
    "$$\n",
    "\n",
    "and then the first $r$ columns of $U$ are given by\n",
    "\n",
    "$$\n",
    "u_j=\\frac{1}{\\sigma_j}Av_j.\n",
    "$$\n",
    "\n",
    "The geometry of the linear transformation associated with $A$ is the composition of an isometry, a stretching, and another isometry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
