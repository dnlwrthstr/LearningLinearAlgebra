{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37bbe8b98316a5ba",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc9d98847207b",
   "metadata": {},
   "source": [
    "## Solving Linear Systems with Singular Value Decomposition\n",
    "\n",
    "Systems of linear equations gave rise to linear algebra. But it has been a while since you began to learn this branch of mathematics. You have learned a lot of sophisticated techniques and concepts. At this point, solving a system of equations must seem easy.\n",
    "\n",
    "However, your new favorite tool, **singular value decomposition (SVD)**, has much to say about it. In fact, thanks to SVD, you will look at solving systems of linear equations in a slightly different way.\n",
    "\n",
    "In this topic, you will learn how to calculate the solution of **any** system of equations. But wait—not all systems have solutions, and some even have infinitely many solutions. So how can we talk about *“the solution”* of any system?\n",
    "\n",
    "Get ready to get a little dizzy and develop an infallible tool.\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "In the following, you will work with:\n",
    "\n",
    "- a matrix\n",
    "  $$A \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "- with rank\n",
    "  $$\\operatorname{rank}(A) = r$$\n",
    "\n",
    "- and a vector\n",
    "  $$b \\in \\mathbb{R}^m$$\n",
    "\n",
    "The goal is to understand how SVD allows us to define and compute a meaningful solution to the system\n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "in **all** cases: unique solution, infinitely many solutions, or no exact solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99e6d5fc598f7b",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "As you already know, every matrix has a singular value decomposition (SVD). This decomposition allows you to understand the properties of a matrix deeply. Remarkably, the most useful form of the SVD is the following:\n",
    "\n",
    "$$\n",
    "A = \\sum_{j=1}^{r} \\sigma_j \\, u_j v_j^{T}\n",
    "$$\n",
    "\n",
    "Let us investigate the relationship between this decomposition and the inverse of a matrix.\n",
    "\n",
    "Recall that if $A$ is a square matrix of size $n$ and all of its singular values are positive, then $A$ is invertible. In that case, its inverse is given by:\n",
    "\n",
    "$$\n",
    "A^{-1} = V \\Sigma^{-1} U^{T}\n",
    "       = \\sum_{j=1}^{n} \\frac{1}{\\sigma_j} \\, v_j u_j^{T}\n",
    "$$\n",
    "\n",
    "For a general matrix, only the first $r$ singular values are non-zero. Motivated by the expression above, we define the closest analogue to the inverse of $A$, namely its **pseudoinverse**:\n",
    "\n",
    "$$\n",
    "A^{\\dagger} = \\sum_{j=1}^{r} \\frac{1}{\\sigma_j} \\, v_j u_j^{T}\n",
    "$$\n",
    "\n",
    "Note that the size of $A^{\\dagger}$ is $n \\times m$.\n",
    "\n",
    "Using the matrix form of the SVD, if\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^{T},\n",
    "$$\n",
    "\n",
    "then the pseudoinverse is given by\n",
    "\n",
    "$$\n",
    "A^{\\dagger} = V \\Sigma^{\\dagger} U^{T},\n",
    "$$\n",
    "\n",
    "where $\\Sigma^{\\dagger}$ is the diagonal matrix whose non-zero entries are the reciprocals of the non-zero entries of $\\Sigma$. The zero columns of $\\Sigma$ are moved as rows to the bottom so that $\\Sigma^{\\dagger}$ has size $n \\times m$, whereas $\\Sigma$ has size $m \\times n$.\n",
    "\n",
    "This means that, although not every matrix is invertible, thanks to SVD **every matrix (even rectangular ones) has a pseudoinverse**.\n",
    "\n",
    "As a first application, observe that when the matrix is square of size $n$ and invertible, its pseudoinverse coincides with its inverse. Indeed, in this case,\n",
    "\n",
    "$$\n",
    "\\Sigma^{\\dagger} = \\Sigma^{-1},\n",
    "$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^{\\dagger} A\n",
    "&= (V \\Sigma^{-1} U^{T})(U \\Sigma V^{T}) \\\\\n",
    "&= (V \\Sigma^{-1})(\\Sigma V^{T}) \\\\\n",
    "&= V V^{T} \\\\\n",
    "&= I.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In summary, you have just proved the following result:\n",
    "\n",
    "**If $A$ is invertible, then**\n",
    "$$\n",
    "A^{\\dagger} = A^{-1}.\n",
    "$$\n",
    "\n",
    "Before delving into the properties of the pseudoinverse, let us see how it can be computed easily in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5c4ba18417627",
   "metadata": {},
   "source": [
    "## Calculation\n",
    "\n",
    "You already have plenty of practice calculating the SVD and are familiar with the advantages it brings. You can use this decomposition to construct the pseudoinverse easily as you just saw. Let's see an example. Take the matrix:\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 & 1\\\\\n",
    "2 & 2 & -2 & -2\\\\\n",
    "3 & -3 & 3 & -3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "It's clear that it isn't invertible, but as it always has an SVD, it has a pseudoinverse. First of all, an SVD for $A$ is given by:\n",
    "\n",
    "$$\n",
    "U=\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\Sigma=\n",
    "\\begin{pmatrix}\n",
    "6 & 0 & 0 & 0\\\\\n",
    "0 & 4 & 0 & 0\\\\\n",
    "0 & 0 & 2 & 0\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "V=\\frac12\n",
    "\\begin{pmatrix}\n",
    "1 & -1 & 1 & -1\\\\\n",
    "1 & 1 & -1 & -1\\\\\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & -1 & -1 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This means that:\n",
    "\n",
    "$$\n",
    "\\Sigma^\\dagger=\n",
    "\\begin{pmatrix}\n",
    "\\frac16 & 0 & 0\\\\\n",
    "0 & \\frac14 & 0\\\\\n",
    "0 & 0 & \\frac12\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "A^\\dagger\n",
    "=\n",
    "V\\Sigma^\\dagger U^T\n",
    "=\n",
    "\\frac12\n",
    "\\begin{pmatrix}\n",
    "1 & -1 & 1 & -1\\\\\n",
    "1 & 1 & -1 & -1\\\\\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & -1 & -1 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\frac16 & 0 & 0\\\\\n",
    "0 & \\frac14 & 0\\\\\n",
    "0 & 0 & \\frac12\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac14 & \\frac18 & \\frac1{12}\\\\\n",
    "\\frac14 & \\frac18 & -\\frac1{12}\\\\\n",
    "\\frac14 & -\\frac18 & \\frac1{12}\\\\\n",
    "\\frac14 & -\\frac18 & -\\frac1{12}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Up to this point, you know that every matrix has a pseudoinverse, which is easy to calculate after obtaining the SVD, and that when the original matrix is invertible, it reduces to the inverse. The pseudoinverse allows you to find a unique solution. Now, you will explore important applications of this new tool: systems of equations where there is no unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a9f796eda1e8c",
   "metadata": {},
   "source": [
    "## The best solution among infinite\n",
    "\n",
    "If you try to solve the system\n",
    "$Ax=b$ given by\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "2 & 3 & 4\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "b=\n",
    "\\begin{pmatrix}\n",
    "2\\\\\n",
    "-1\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "you should get that the solutions are\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "-8\\\\\n",
    "5\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\lambda\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "-2\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\qquad \\text{for any } \\lambda \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "At first glance, it is not obvious which of all the possible solutions is the simplest. For instance,\n",
    "$\\lambda = 100$ is perfectly valid, but the associated solution would look like\n",
    "\n",
    "$$\n",
    "(92,-195,100)^T,\n",
    "$$\n",
    "\n",
    "quite ugly. The pseudoinverse can help:\n",
    "\n",
    "$$\n",
    "A^\\dagger\n",
    "=\n",
    "\\frac13\n",
    "\\begin{pmatrix}\n",
    "-11 & 2\\\\\n",
    "4 & -1\\\\\n",
    "7 & -2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "If you were to calculate\n",
    "\n",
    "$$\n",
    "x = A^\\dagger b =\n",
    "\\begin{pmatrix}\n",
    "-5\\\\\n",
    "-1\\\\\n",
    "3\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "then $x$ is a solution of the system since $Ax=b$.\n",
    "\n",
    "But this solution has a curious quality. It is the smallest in the sense that its norm is lower than any other solution. This means that it is the solution closest to the origin.\n",
    "\n",
    "---\n",
    "\n",
    "## The smallest solution\n",
    "\n",
    "Best of all, this is generally true:\n",
    "\n",
    "If the system $Ax=b$ has infinite solutions, then\n",
    "\n",
    "$$\n",
    "x = A^\\dagger b\n",
    "$$\n",
    "\n",
    "is a solution with the property that for any other solution $y$ it holds that\n",
    "\n",
    "$$\n",
    "\\|x\\| \\le \\|y\\|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707d7973479e2b6",
   "metadata": {},
   "source": [
    "## The closest thing to a solution\n",
    "\n",
    "Let's face it: very few rectangular systems of equations have solutions. This is even more common when there are more equations than variables. In such a case, there are too many conditions that a few variables have to meet.\n",
    "\n",
    "Remind that the linear system\n",
    "$Ax=b$ can be rewritten as\n",
    "\n",
    "$$\n",
    "x_1 a_1 + x_2 a_2 + \\cdots + x_n a_n = b,\n",
    "$$\n",
    "\n",
    "where $a_1,a_2,\\dots,a_n$ are the columns of $A$. This means that the system has a solution only when $b$ belongs to the subspace generated by the columns of $A$, denoted by $\\operatorname{Im}(A)$. When there is no solution, you could try to approximate $b$ as much as possible with some element of $\\operatorname{Im}(A)$. That is, the vector $Ax\\in\\operatorname{Im}(A)$ such that\n",
    "\n",
    "$$\n",
    "\\|Ax-b\\|\\le \\|Ay-b\\|\n",
    "$$\n",
    "\n",
    "for any other $y\\in\\mathbb{R}^n$.\n",
    "\n",
    "---\n",
    "\n",
    "## The best approximation when there's no solution\n",
    "\n",
    "The picture suggests that the vector $b-Ax$ is orthogonal to $\\operatorname{Im}(A)$. You should think of the vector $Ax$ as the perpendicular projection of $b$ onto $\\operatorname{Im}(A)$. It might seem tricky to find it, but the pseudoinverse to the rescue!\n",
    "\n",
    "Consider the linear system $Ax=b$ and define\n",
    "\n",
    "$$\n",
    "x = A^\\dagger b.\n",
    "$$\n",
    "\n",
    "Then, for any other $y\\in\\mathbb{R}^n$,\n",
    "\n",
    "$$\n",
    "\\|Ax-b\\| \\le \\|Ay-b\\|.\n",
    "$$\n",
    "\n",
    "This optimization technique is known as least squares, and you will explore it in the next topic. For now, let's look at a simple example. Suppose you want to solve the system given by\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 1 & 0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "b=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This system has no solution, so the best strategy is to approximate $b$ with the projection of $b$ onto $\\operatorname{Im}(A)$. For this, verify that\n",
    "\n",
    "$$\n",
    "A^\\dagger\n",
    "=\n",
    "\\frac13\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 1\\\\\n",
    "-1 & 2 & 1\\\\\n",
    "-1 & 2 & 1\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "x = A^\\dagger b\n",
    "=\n",
    "\\frac13\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "Ax\n",
    "=\n",
    "\\frac13\n",
    "\\begin{pmatrix}\n",
    "4\\\\\n",
    "2\\\\\n",
    "2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Therefore, the best approximation to $b$ among all vectors of $\\operatorname{Im}(A)$ is given by\n",
    "\n",
    "$$\n",
    "Ax=\\frac13(4,2,2)^T,\n",
    "$$\n",
    "\n",
    "and this would be your best estimation of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5b694c3c407da",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Every matrix $A$ has a pseudoinverse given by\n",
    "\n",
    "$$\n",
    "A^\\dagger\n",
    "=\n",
    "\\sum_{j=1}^{r} \\frac{1}{\\sigma_j}\\, v_j u_j^T .\n",
    "$$\n",
    "\n",
    "When the matrix $A$ is invertible,\n",
    "\n",
    "$$\n",
    "A^\\dagger = A^{-1}.\n",
    "$$\n",
    "\n",
    "If the linear system $Ax=b$ has infinitely many solutions, then\n",
    "\n",
    "$$\n",
    "x = A^\\dagger b\n",
    "$$\n",
    "\n",
    "is the shortest one.\n",
    "\n",
    "If the linear system $Ax=b$ has no solutions, then\n",
    "\n",
    "$$\n",
    "x = A^\\dagger b\n",
    "$$\n",
    "\n",
    "gives you the closest thing to a solution in the sense that for any other $y \\in \\mathbb{R}^n$,\n",
    "$Ax$ is closer to $b$ than $Ay$ is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
