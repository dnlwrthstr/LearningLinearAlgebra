{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8638056018d9b9ae",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Least squares\n",
    "\n",
    "In the world of science and engineering, you often encounter data that appears chaotic or noisy. However, behind this apparent confusion may lie fundamental patterns and relationships you can reveal using the techniques you’ve developed. In this topic, you will explore how the pseudoinverse becomes the driving engine behind the least squares method, an essential technique for fitting models to data.\n",
    "\n",
    "You’ll take advantage of your mathematical skills to interpret the least squares problem in matrix form. Although the problem arises naturally in 2 dimensions, your knowledge of linear algebra will allow you to reformulate it in its most general context. You’ll develop a criterion to determine the best possible solution for the pseudoinverse, so now you’ll only reap the fruits of your effort. It’s time to transition from theory to practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ad61254158a27",
   "metadata": {},
   "source": [
    "## A cloud of points\n",
    "\n",
    "Suppose you have $n$ data points in the plane $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$. Suppose these data represent the money invested in advertising and the corresponding profits earned by various companies in some industry. A reasonable assumption would be that the more you invest in advertising, the more profits are generated, so you could start your research assuming that the relationship between the two has a linear trend:\n",
    "\n",
    "![scatter_plot.png](img/scatter_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc57f75cb83f93",
   "metadata": {},
   "source": [
    "In an ideal case, all the points are on the same line. As any line is determined by its slope\n",
    "$m$ and intercept $b$, the problem translates into finding the numbers $m$ and $b$ satisfying the linear system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_1 &= m x_1 + b \\\\\n",
    "y_2 &= m x_2 + b \\\\\n",
    "&\\;\\vdots \\\\\n",
    "y_n &= m x_n + b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But there are too many equations with only two variables. So, it's unlikely that the system has a solution. Geometrically, this means there's no line that fits the raw data perfectly. The best line to describe this dataset isn't the one going through the points but the one that outlines the way this set is directed or oriented its increasing/decreasing pattern in the best manner. As\n",
    "$y$ is expressed in terms of $y$, we say that\n",
    "$x$ is a **predictor** and that $y$ is the **target**. Before thinking about a way to attack this problem, let's look at this problem in larger dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16443f7f0cbf10e",
   "metadata": {},
   "source": [
    "## The general problem\n",
    "\n",
    "Now, suppose you're given $n$ data points in the space $(x_1, y_1, z1). (x_2, y_2, z_2), ..., (x_n, y_n, z_n)$. Your goal is to find a plane that best approximates all these points:\n",
    "\n",
    "![3d_scatter.png](img/3d_scatter.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011a4429007e111",
   "metadata": {},
   "source": [
    "Analogously to the two-dimensional problem, this poses a system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 &= m x_1 + l y_1+ b \\\\\n",
    "y_2 &= m x_2 +l y_2 + b \\\\\n",
    "&\\;\\vdots \\\\\n",
    "z_n &= m x_n + ly_n + b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So, the problem reduces to find the values of $m$, $l$ and $b$. When $n$ is a large number, it's nearly impossible that the system could have a solution. Again, since $z$ is expressed in terms of $x$ and $y$, $z$ is called the **target** while $x$ and $y$ are **predictors**.\n",
    "\n",
    "You already know linear algebra, so you can work with the most general scenario involving many variables and linear equations. In general, you have\n",
    "$n$ data points with $p$ predictors $x_1, x_2,..., x_p$ and a target $Y$.\n",
    "\n",
    "Let $x_{ij}$ be the $i$-th observation of the $j$-th predictor and $y_i$ the $i$-th observation of the target for every $1 \\le i \\le n$ and $1 \\le j \\le p$. This implies that the data points have the form  $(x_{11}, x_{12}, \\ldots, x_{1p}, y_1),\\quad\n",
    "(x_{21}, x_{22}, \\ldots, x_{2p}, y_2),\\quad \\ldots,\\quad(x_{n1}, x_{n2}, \\ldots, x_{np}, y_n)$.\n",
    "\n",
    "Under these conditions, the initial problem can be written as a linear system of equations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_1 &= \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} + \\cdots + \\beta_p x_{1p}, \\\\\n",
    "y_2 &= \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\cdots + \\beta_p x_{2p}, \\\\\n",
    "&\\ \\vdots \\\\\n",
    "y_n &= \\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\cdots + \\beta_p x_{np}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But as in lower dimensions, it is very unlikely that such a system (where $n$ is usually much larger than $p$) has any solutions. But not everything is lost. Let's see how you can get your way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9cc41c08f997fb",
   "metadata": {},
   "source": [
    "## An optimality criterion\n",
    "\n",
    "You're working with a lot of variables and equations at the same time, and it's easy to get confused. Time to introduce some matrices!\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Thanks to them, the linear system becomes a simple matrix equation:\n",
    "\n",
    "$$\n",
    "X \\beta = y.\n",
    "$$\n",
    "\n",
    "So, for any proposed vector of solutions $\\hat{\\beta}$, you get an approximation\n",
    "$\\hat{y} = X \\hat{\\beta}$ to $y$.\n",
    "Since the system has no solution, it's clear that $\\hat{y} \\ne y$, so the distance\n",
    "$\\lVert y - \\hat{y} \\rVert$ between both vectors is non-zero. You can think of this distance as an error associated with $\\hat{\\beta}$.\n",
    "\n",
    "In summary, for any $\\hat{\\beta}$, its associated **estimation error** is:\n",
    "\n",
    "$$\n",
    "e(\\hat{\\beta}) = \\lVert y - X \\hat{\\beta} \\rVert.\n",
    "$$\n",
    "\n",
    "The smaller the error, the better the approximation. This suggests that the best parameter vector $\\hat{\\beta}$ is the one that minimizes\n",
    "$e(\\hat{\\beta}) = \\lVert y - X \\hat{\\beta} \\rVert$.\n",
    "\n",
    "But then you might be wondering if there even exists a unique vector with this property, and worse, how can you find it?\n",
    "Don't worry, this is precisely the problem that the pseudoinverse solves!\n",
    "\n",
    "![regression.png](img/regression.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dffff7fbc3a13f",
   "metadata": {},
   "source": [
    "Recall you are given $X$ and $y$, think about them as fixed.\n",
    "Your goal is to compute $\\hat{\\beta}$.\n",
    "\n",
    "With it, you can estimate the target values as\n",
    "\n",
    "$$\n",
    "\\hat{y} = X \\hat{\\beta}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92782a491a4ba3",
   "metadata": {},
   "source": [
    "## The best approximation\n",
    "\n",
    "Well, as you already know, the vector you're looking for is simply:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = X^{\\dagger} y\n",
    "$$\n",
    "\n",
    "where $X^{\\dagger}$ is the pseudoinverse of $X$.\n",
    "This is because\n",
    "$$\\lVert X \\hat{\\beta} - y \\rVert \\le \\lVert X \\beta - y \\rVert$$\n",
    "for any other $\\beta$.\n",
    "In our current problem, this means that\n",
    "$$e(\\hat{\\beta}) \\le e(\\beta).$$\n",
    "\n",
    "Once you've computed the best parameter vector, you can estimate the target:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X X^{\\dagger} y = X \\hat{\\beta}.\n",
    "$$\n",
    "\n",
    "Before putting the theory to work, let's discuss one more point.\n",
    "The vector $\\hat{y}$ is the closest to $y$ among all the vectors in the column space of $X$.\n",
    "Actually, this is the orthogonal projection of $\\hat{y}$ onto the column space of $X$.\n",
    "Thus, by multiplying the vector $\\hat{y}$ by the matrix $X X^{\\dagger}$ you obtain its projection.\n",
    "\n",
    "For this reason,\n",
    "$$H = X X^{\\dagger}$$\n",
    "is known as the **projection matrix**.\n",
    "\n",
    "- In the standard approach, the optimization process is carried out through derivatives.\n",
    "  Specifically, the error function is minimized through the second derivative, which is tedious.\n",
    "\n",
    "- Furthermore, in the standard approach, the error is not minimized directly, but rather the *quadratic* error —\n",
    "  this is done because the quadratic error is easier to derive.\n",
    "\n",
    "- For the standard approach to work, it is usually assumed that the columns of $X$ are linearly independent.\n",
    "  In these circumstances, the projection matrix is reduced to $y$.\n",
    "  When you have many predictors, this can easily fail, and other problems arise.\n",
    "  In our strategy, we do not suffer from this problem, and the projection matrix is much easier to remember.\n",
    "\n",
    "- The least squares method is usually derived by the usage of calculus,\n",
    "  but we're introducing an alternative way of solving the problem, which is more straightforward.\n",
    "  Actually, in statistics, this method allows you to go deeper.\n",
    "  This allows you to estimate other parameters, perform hypothesis tests,\n",
    "  and evaluate the quality of fit — all in a unified way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ba20ac5029da7",
   "metadata": {},
   "source": [
    "## The best line\n",
    "\n",
    "Let's start off with a simple example in two dimensions.\n",
    "Suppose the data points are\n",
    "$(-1, 3),\\ (0, 1),\\ (1, -1),\\ (2, 2)$.\n",
    "\n",
    "In order to find the best estimation, the first step is to identify the data matrix, the target values, and the parameters:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "1 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "3 \\\\\n",
    "1 \\\\\n",
    "-1 \\\\\n",
    "2\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "b \\\\\n",
    "m\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "So, you're trying to find the closest thing to a solution for the system\n",
    "$$X \\beta = y.$$\n",
    "\n",
    "Here, the key step is to compute the pseudoinverse of $X$.\n",
    "Although it isn't necessary, you can also calculate the projection matrix:\n",
    "\n",
    "$$\n",
    "X^{\\dagger}\n",
    "= \\frac{1}{10}\n",
    "\\begin{pmatrix}\n",
    "4 & 3 & 2 & 1 \\\\\n",
    "-3 & -1 & 1 & 3\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "H = X X^{\\dagger}\n",
    "= \\frac{1}{10}\n",
    "\\begin{pmatrix}\n",
    "7 & 4 & 1 & -2 \\\\\n",
    "4 & 3 & 2 & 1 \\\\\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "-2 & 1 & 4 & 7\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Now you have everything to calculate the parameter $\\hat{\\beta}$ and the corresponding estimate of $y$:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}\n",
    "= X^{\\dagger} y\n",
    "= \\frac{1}{2}\n",
    "\\begin{pmatrix}\n",
    "3 \\\\\n",
    "-1\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\hat{y}\n",
    "= X \\hat{\\beta}\n",
    "= \\frac{1}{2}\n",
    "\\begin{pmatrix}\n",
    "4 \\\\\n",
    "3 \\\\\n",
    "2 \\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "In this case, the error is\n",
    "\n",
    "$$\n",
    "e(\\hat{\\beta}) = \\lVert y - X \\hat{\\beta} \\rVert \\approx 2.73861.\n",
    "$$\n",
    "\n",
    "Then, the error associated with any other beta vector is greater than or equal to this value.\n",
    "Geometrically, the slope of the best line is $-\\frac{1}{2}$ and its intercept is $\\frac{3}{2}$.\n",
    "\n",
    "![best_line.png](img/best_line.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29c752c5d09f2e",
   "metadata": {},
   "source": [
    "## The best plane\n",
    "\n",
    "Now, let's move on to a slightly more realistic example.\n",
    "There is more than one predictor, and the pseudoinverse doesn't look pretty.\n",
    "The data points are\n",
    "$(-3, -6, 16),\\ (1, -4, 13),\\ (0, 1, -3),\\ (2, 5, -13),\\ (5, 7, -18).$\n",
    "\n",
    "As before, the required pieces are:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "1 & -3 & -6 \\\\\n",
    "1 & 1 & -4 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 2 & 5 \\\\\n",
    "1 & 5 & 7\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "16 \\\\\n",
    "13 \\\\\n",
    "-3 \\\\\n",
    "-13 \\\\\n",
    "-18\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "b \\\\\n",
    "m \\\\\n",
    "l\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The corresponding pseudoinverse is\n",
    "\n",
    "$$\n",
    "X^{\\dagger} =\n",
    "\\begin{pmatrix}\n",
    "0.317 & 0.0539 & 0.294 & 0.258 & 0.0764 \\\\\n",
    "-0.117 & 0.230 & -0.132 & -0.108 & 0.127 \\\\\n",
    "-0.000357 & -0.140 & 0.0621 & 0.0835 & -0.00571\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This implies that the projection matrix is then:\n",
    "\n",
    "$$\n",
    "H = X X^{\\dagger}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0.671 & 0.202 & 0.317 & 0.0814 & -0.271 \\\\\n",
    "0.202 & 0.842 & -0.0857 & -0.184 & 0.226 \\\\\n",
    "0.317 & -0.0857 & 0.357 & 0.342 & 0.0707 \\\\\n",
    "0.0814 & -0.184 & 0.342 & 0.459 & 0.302 \\\\\n",
    "-0.271 & 0.226 & 0.0707 & 0.302 & 0.672\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Finally, the parameter vector and the corresponding best approximation are:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}\n",
    "= X^{\\dagger} y\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0.165 \\\\\n",
    "0.629 \\\\\n",
    "-2.99\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\hat{y}\n",
    "= X \\hat{\\beta}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "16.2 \\\\\n",
    "12.7 \\\\\n",
    "-2.82 \\\\\n",
    "-13.5 \\\\\n",
    "-17.6\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "![3d_regression.png](img/3d_regression.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6913cda88b6459",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- In the least squares problem, you have $n$ data points of $p$ predictors and a target variable\n",
    "  $(x_{11}, x_{12}, \\ldots, x_{1p}, y_1),\\ (x_{21}, x_{22}, \\ldots, x_{2p}, y_2),\\ \\ldots,\\ (x_{n1}, x_{n2}, \\ldots, x_{np}, y_n)$.\n",
    "\n",
    "- Your meaningful goal is to find some linear function describing your data.\n",
    "  The closest thing to a solution for the linear system $X\\beta = y$ is an instrument you use to reach this goal.\n",
    "\n",
    "- The best solution is the vector $\\hat{\\beta}$ whose error\n",
    "  $e(\\hat{\\beta}) = \\lVert y - X \\hat{\\beta} \\rVert$\n",
    "  is as small as possible.\n",
    "\n",
    "- The best solution is given by $\\hat{\\beta} = X^{\\dagger} y$ and the estimation for the target is\n",
    "  $\\hat{y} = X \\hat{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57318153ff14f5ef",
   "metadata": {},
   "source": [
    "## Example 1 - Assessing the error\n",
    "\n",
    "Consider the least squares problem with:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "1 & -2 & 1 \\\\\n",
    "1 & 1  & 2 \\\\\n",
    "1 & 3  & 0 \\\\\n",
    "1 & 0  & 1\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "-2 \\\\\n",
    "0 \\\\\n",
    "3 \\\\\n",
    "2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The pseudoinverse is given by:\n",
    "\n",
    "$$\n",
    "X^{\\dagger}\n",
    "=\n",
    "\\frac{1}{22}\n",
    "\\begin{pmatrix}\n",
    "13 & -10 & 12 & 7 \\\\\n",
    "-5 & 3 & 3 & -1 \\\\\n",
    "-5 & 14 & -8 & -1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Compute the error with two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7790498c4a94496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:23:02.643216Z",
     "start_time": "2026-01-19T21:23:02.636159Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [1, -2, 1],\n",
    "    [1,  1, 2],\n",
    "    [1,  3, 0],\n",
    "    [1,  0, 1]\n",
    "], dtype=float)\n",
    "\n",
    "y = np.array([\n",
    "    -2,\n",
    "     0,\n",
    "     3,\n",
    "     2\n",
    "], dtype=float)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_dagger = (1/22) * np.array([\n",
    "    [13, -10, 12,  7],\n",
    "    [-5,   3,  3, -1],\n",
    "    [-5,  14, -8, -1]\n",
    "], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c9be6ada8114c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:23:02.652760Z",
     "start_time": "2026-01-19T21:23:02.643646Z"
    }
   },
   "outputs": [],
   "source": [
    "H = X @ X_dagger\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7640b554c39cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-19T21:23:02.910075Z"
    }
   },
   "outputs": [],
   "source": [
    "BETA = X_dagger @ y\n",
    "BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfab57c2591dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = X @ BETA\n",
    "y_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1e9d5a2caff73",
   "metadata": {},
   "source": [
    "## Visualization with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460deeacd47d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paste the extracted data here (matrix provided below)\n",
    "data = np.array([\n",
    "[-2.000000, -6.000000],\n",
    "[-0.439009, -5.976424],\n",
    "[-0.127700, -5.834971],\n",
    "[-1.172808, -4.821218],\n",
    "[0.312579, -4.797642],\n",
    "[1.512560, -4.750491],\n",
    "[0.887294, -4.703340],\n",
    "[1.606397, -4.656189],\n",
    "[2.693439, -4.632614],\n",
    "[1.324178, -4.561887],\n",
    "[2.474493, -4.468010],\n",
    "[0.949421, -4.444434],\n",
    "[4.531158, -4.374494],\n",
    "[2.405266, -4.280618],\n",
    "[1.928322, -4.256358],\n",
    "[4.030131, -4.209207],\n",
    "[2.908761, -4.114715],\n",
    "[4.313298, -4.091139],\n",
    "[3.027142, -3.996647],\n",
    "[2.165679, -3.926322],\n",
    "[1.427074, -3.879171],\n",
    "[3.523347, -3.855595],\n",
    "[2.588884, -3.761103],\n",
    "[1.540606, -3.690778],\n",
    "[3.735869, -3.667202],\n",
    "[1.769497, -3.572710],\n",
    "[4.423119, -3.502385],\n",
    "[3.539413, -3.431746],\n",
    "[1.984379, -3.384595],\n",
    "[0.826979, -3.337444],\n",
    "[4.024789, -3.266718],\n",
    "[2.941485, -3.196393],\n",
    "[1.553298, -3.148927],\n",
    "[3.341243, -3.125351],\n",
    "[4.476124, -3.078200],\n",
    "[2.247586, -2.983709],\n",
    "[3.566732, -2.912982],\n",
    "[1.041499, -2.818491],\n",
    "[4.294381, -2.771340],\n",
    "[2.559163, -2.724189],\n",
    "[1.862612, -2.653462],\n",
    "[3.732475, -2.606311],\n",
    "[4.499689, -2.535585],\n",
    "[2.462116, -2.488120],\n",
    "[3.275040, -2.417394],\n",
    "[1.009806, -2.370243],\n",
    "[4.273672, -2.299517],\n",
    "[2.979778, -2.228790],\n",
    "[3.712915, -2.157750],\n",
    "[1.617897, -2.086710],\n",
    "[2.308622, -2.015984],\n",
    "[3.097953, -1.945257],\n",
    "[4.320262, -1.827925],\n",
    "[2.451994, -1.757199],\n",
    "[3.516693, -1.686159],\n",
    "[1.085814, -1.615433],\n",
    "[4.099907, -1.544706],\n",
    "[2.773748, -1.473980],\n",
    "[3.214690, -1.402940],\n",
    "[1.674512, -1.331900],\n",
    "[2.358693, -1.261174],\n",
    "[4.472711, -1.143842],\n",
    "[3.288929, -1.072802],\n",
    "[2.020728, -1.001762],\n",
    "[0.857807, -0.954611],\n",
    "[4.280152, -0.812518],\n",
    "[2.932150, -0.765367],\n",
    "[3.510024, -0.694640],\n",
    "[1.110535, -0.647489],\n",
    "[2.348415, -0.576763],\n",
    "[4.401259, -0.459431],\n",
    "[3.157934, -0.388391],\n",
    "[1.542320, -0.341240],\n",
    "[2.720169, -0.270514],\n",
    "[3.961604, -0.153182],\n",
    "[2.566694, -0.082142],\n",
    "[1.635979, -0.011102],\n",
    "[4.443691, 0.082773],\n",
    "[3.392153, 0.153499],\n",
    "[2.266603, 0.247991],\n",
    "[4.089724, 0.365323],\n",
    "[3.040387, 0.436049],\n",
    "[1.887111, 0.507089],\n",
    "[2.931187, 0.601580],\n",
    "[3.771291, 0.696072],\n",
    "[4.434355, 0.767112],\n",
    "[2.580735, 0.861604],\n",
    "[1.523961, 0.956095],\n",
    "[3.281575, 1.050587],\n",
    "[2.139517, 1.145079],\n",
    "[3.712813, 1.239570]\n",
    "])\n",
    "\n",
    "# Split into x, y\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x, y, color=\"red\", s=20)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Reconstructed Scatterplot from Extracted Image Data\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df7f6bfa806048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Generate sample data\n",
    "# -----------------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "n = 50\n",
    "x = np.random.uniform(-1, 1, n)\n",
    "y = np.random.uniform(-1, 1, n)\n",
    "z = np.random.uniform(-5, 5, n)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configure figure\n",
    "# -----------------------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# White background\n",
    "fig.patch.set_facecolor('blue')\n",
    "ax.set_facecolor('blue')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Scatter plot\n",
    "# -----------------------------------------------------------------------------\n",
    "ax.scatter(x, y, z, color='purple', s=80)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Axis labels\n",
    "# -----------------------------------------------------------------------------\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_zlabel('z', fontsize=12)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Match viewing angle\n",
    "# -----------------------------------------------------------------------------\n",
    "ax.view_init(elev=15, azim=-75)\n",
    "\n",
    "# Axis limits similar to your plot\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_zlim(-5, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe214fb6b2665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)   # for reproducibility\n",
    "\n",
    "n = 100\n",
    "x = np.random.uniform(-2, 5, n)          # x values in the same range as your plot\n",
    "noise = np.random.normal(0, 1.2, n)      # random noise\n",
    "y = 1.1 * x + 0.5 + noise                # linear relationship with noise\n",
    "\n",
    "# Reshape for sklearn\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "# 2. Fit a linear regression line\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, color='red', s=20)\n",
    "plt.plot(x, y_pred, linewidth=2)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f55db51ade73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# DATA FROM THE EXAMPLE\n",
    "# ----------------------------------------------------------\n",
    "X_points = np.array([-1, 0, 1, 2])\n",
    "y_points = np.array([3, 1, -1, 2])\n",
    "\n",
    "# Best-fit parameters from your derivation\n",
    "b = 1.5   # intercept\n",
    "m = -0.5  # slope\n",
    "\n",
    "# Compute fitted line over a smooth grid\n",
    "x_line = np.linspace(-2, 3, 200)\n",
    "y_line = b + m * x_line\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter points\n",
    "plt.scatter(X_points, y_points, color='purple', s=120, label='Data points')\n",
    "\n",
    "# Best-fit line\n",
    "plt.plot(x_line, y_line, color='red', linewidth=2.5, label='Best-fit line')\n",
    "\n",
    "# Aesthetic details\n",
    "plt.title(\"Best Line from Pseudoinverse Solution\", fontsize=20)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlim(-2, 3)\n",
    "plt.ylim(-3, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3a3cf483014d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401  (needed for 3D)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# DATA: points (x, y, z)\n",
    "# ----------------------------------------------------------\n",
    "points = np.array([\n",
    "    [-3, -6, 16],\n",
    "    [ 1, -4, 13],\n",
    "    [ 0,  1, -3],\n",
    "    [ 2,  5, -13],\n",
    "    [ 5,  7, -18],\n",
    "])\n",
    "\n",
    "x = points[:, 0]\n",
    "y = points[:, 1]\n",
    "z = points[:, 2]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PLANE PARAMETERS  z = b + m x + l y\n",
    "# (from your pseudoinverse solution)\n",
    "# ----------------------------------------------------------\n",
    "b = 0.165\n",
    "m = 0.629\n",
    "l = -2.99\n",
    "\n",
    "# Fitted z values (projection of points onto the plane)\n",
    "z_hat = b + m * x + l * y\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CREATE PLANE GRID\n",
    "# ----------------------------------------------------------\n",
    "x_grid = np.linspace(x.min() - 1, x.max() + 1, 30)\n",
    "y_grid = np.linspace(y.min() - 1, y.max() + 1, 30)\n",
    "Xg, Yg = np.meshgrid(x_grid, y_grid)\n",
    "Zg = b + m * Xg + l * Yg\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Best-fit plane\n",
    "ax.plot_surface(Xg, Yg, Zg, alpha=0.3, edgecolor='none', cmap='viridis')\n",
    "\n",
    "# Original data points\n",
    "ax.scatter(x, y, z, color='purple', s=80, label='Data points')\n",
    "\n",
    "# Projected points on the plane\n",
    "ax.scatter(x, y, z_hat, color='blue', marker='x', s=80, label='Projections on plane')\n",
    "\n",
    "# Residual vectors (from projection on plane to actual point)\n",
    "for xi, yi, zi, zhi in zip(x, y, z, z_hat):\n",
    "    ax.plot([xi, xi], [yi, yi], [zhi, zi],\n",
    "            color='black', linewidth=1.5)\n",
    "\n",
    "# Labels and view\n",
    "ax.set_title(\"Best-fit plane with residual vectors\", fontsize=16)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.view_init(elev=20, azim=-60)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67e473928741a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# DATA\n",
    "# ----------------------------------------------------------\n",
    "points = np.array([\n",
    "    [-3, -6, 16],\n",
    "    [ 1, -4, 13],\n",
    "    [ 0,  1, -3],\n",
    "    [ 2,  5, -13],\n",
    "    [ 5,  7, -18],\n",
    "])\n",
    "\n",
    "x = points[:, 0]\n",
    "y = points[:, 1]\n",
    "z = points[:, 2]\n",
    "\n",
    "# Plane parameters  z = b + m x + l y\n",
    "b = 0.165\n",
    "m = 0.629\n",
    "l = -2.99\n",
    "\n",
    "# Plane grid\n",
    "xg = np.linspace(x.min() - 1, x.max() + 1, 40)\n",
    "yg = np.linspace(y.min() - 1, y.max() + 1, 40)\n",
    "Xg, Yg = np.meshgrid(xg, yg)\n",
    "Zg = b + m * Xg + l * Yg\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Solid plane in brown, slight transparency, grid lines visible\n",
    "ax.plot_surface(\n",
    "    Xg, Yg, Zg,\n",
    "    color=(0.6, 0.3, 0.0, 1.0),          # brown\n",
    "    edgecolor='black',\n",
    "    linewidth=0.3,\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Data points\n",
    "ax.scatter(x, y, z, s=60, color='#4a72b2')  # muted blue\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel(\"First predictor\", fontsize=12)\n",
    "ax.set_ylabel(\"Second predictor\", fontsize=12)\n",
    "ax.set_zlabel(\"Target\", fontsize=12)\n",
    "\n",
    "# Clean, muted axes\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.grid(color='gray', alpha=0.3)\n",
    "\n",
    "# Camera angle similar to your image\n",
    "ax.view_init(elev=20, azim=-120)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
