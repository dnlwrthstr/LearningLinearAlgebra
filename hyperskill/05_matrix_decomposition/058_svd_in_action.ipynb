{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723603c0293c3212",
   "metadata": {},
   "source": [
    "# SVD in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c2b57701a1036",
   "metadata": {},
   "source": [
    "What happens after calculating the singular value decomposition (SVD) of a matrix? In this topic, you'll explore the main applications of this decomposition. You'll finally get a geometric interpretation of the transpose and easily compute the orthonormal basis of spaces closely related to the matrix.\n",
    "\n",
    "You'll also develop an alternative form of the SVD that allows you to progressively rebuild any matrix and accurately approximate it.\n",
    "\n",
    "In the following topic, you'll be working with an $m\\times n$ matrix $A$ of rank $r$ with SVD given by\n",
    "\n",
    "$$\n",
    "A = U\\Sigma V^T.\n",
    "$$\n",
    "\n",
    "As before, the columns of its factors are\n",
    "\n",
    "$$\n",
    "U = [\\,u_1 \\;\\; u_2 \\;\\; \\cdots \\;\\; u_m\\,],\n",
    "\\qquad\n",
    "V = [\\,v_1 \\;\\; v_2 \\;\\; \\cdots \\;\\; v_n\\,].\n",
    "$$\n",
    "\n",
    "The singular values are ordered non-increasingly:\n",
    "\n",
    "$$\n",
    "\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r > 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e488f926df8c7b1",
   "metadata": {},
   "source": [
    "## The geometry of the inverse and the transpose\n",
    "\n",
    "Here comes the first application of singular values. When all of them are different from zero and $A$ is square, then it is important to note that $\\Sigma^{-1}$ is a diagonal matrix whose entries are the multiplicative inverses of the singular values. As a result, $A$ is invertible and its inverse is\n",
    "\n",
    "$$\n",
    "A^{-1}=V\\Sigma^{-1}U^T.\n",
    "$$\n",
    "\n",
    "All the pieces of this decomposition are already known. Therefore, finding $A^{-1}$ is reduced to computing $\\Sigma^{-1}$ and performing two matrix multiplications.\n",
    "\n",
    "Have you ever noticed that we still do not have a geometric interpretation of the transpose? Using the SVD, it becomes clear that\n",
    "\n",
    "$$\n",
    "A^T=V\\Sigma^TU^T.\n",
    "$$\n",
    "\n",
    "Compare the decompositions of $A^{-1}$ and $A^T$. They are very similar. Geometrically, they undo the transformation\n",
    "\n",
    "$$\n",
    "A=U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "but in the opposite order:\n",
    "\n",
    "1. First, they apply $U^T$ to neutralize the effect of $U$.\n",
    "2. Then, they stretch the resulting space.\n",
    "3. Finally, they counteract $V^T$ by applying $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c328de1da7b1c",
   "metadata": {},
   "source": [
    "\n",
    "![svd_geometrie.png](img/svd_geometrie.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4108df2fced2e",
   "metadata": {},
   "source": [
    "But the second step is the actual difference between $A^{-1}$ and $A^T$. While the former undoes the stretch of $A$, the latter simply stretches by the same amount. Thus, you can roughly think of $A^T$ as rotating in the opposite direction as $A$ but stretching in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b0c4aea4ffde5",
   "metadata": {},
   "source": [
    "## The four fundamental spaces\n",
    "\n",
    "The relationship between $A$, its transpose, and the SVD is deeper than it may first appear. The four fundamental spaces of $A$ are\n",
    "\n",
    "$$\n",
    "\\ker(L_A),\\quad \\operatorname{Im}(L_A),\\quad \\ker(L_{A^T}),\\quad \\operatorname{Im}(L_{A^T}).\n",
    "$$\n",
    "\n",
    "Once the SVD is known, all of these spaces can be reconstructed.\n",
    "\n",
    "\n",
    "### Fundamental subspaces from the SVD\n",
    "\n",
    "If $A=U\\Sigma V^T$ has rank $r$, then\n",
    "\n",
    "- $\\{u_1,u_2,\\dots,u_r\\}$ is an orthonormal basis of $\\operatorname{Im}(L_A)$,\n",
    "- $\\{u_{r+1},u_{r+2},\\dots,u_m\\}$ is an orthonormal basis of $\\ker(L_{A^T})$,\n",
    "- $\\{v_1,v_2,\\dots,v_r\\}$ is an orthonormal basis of $\\operatorname{Im}(L_{A^T})$,\n",
    "- $\\{v_{r+1},v_{r+2},\\dots,v_n\\}$ is an orthonormal basis of $\\ker(L_A)$.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the matrix\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 & 1\\\\\n",
    "2 & 2 & -2 & -2\\\\\n",
    "3 & -3 & 3 & -3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "An SVD of $A$ is given by\n",
    "\n",
    "$$\n",
    "U=\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "\\Sigma=\n",
    "\\begin{pmatrix}\n",
    "6 & 0 & 0\\\\\n",
    "0 & 4 & 0\\\\\n",
    "0 & 0 & 2\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "V=\\frac12\n",
    "\\begin{pmatrix}\n",
    "1 & -1 & 1 & -1\\\\\n",
    "1 & 1 & -1 & -1\\\\\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & -1 & -1 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Since there are three positive singular values, the rank is $r=3$.\n",
    "\n",
    "### Fundamental spaces\n",
    "\n",
    "From $U$ we obtain\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix},\n",
    "\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix},\n",
    "\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "as an orthonormal basis of $\\operatorname{Im}(L_A)$, and\n",
    "\n",
    "$$\n",
    "\\ker(L_{A^T})=\\{0\\}.\n",
    "$$\n",
    "\n",
    "From $V$ we obtain\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\frac12\\begin{pmatrix}1\\\\-1\\\\1\\\\-1\\end{pmatrix},\n",
    "\\frac12\\begin{pmatrix}1\\\\1\\\\-1\\\\-1\\end{pmatrix},\n",
    "\\frac12\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "as an orthonormal basis of $\\operatorname{Im}(L_{A^T})$, and\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\frac12\\begin{pmatrix}1\\\\-1\\\\-1\\\\1\\end{pmatrix}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "as an orthonormal basis of $\\ker(L_A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249efc4ba587809b",
   "metadata": {},
   "source": [
    "## An alternative form of the SVD\n",
    "\n",
    "The SVD can be written as a sum of rank-1 matrices:\n",
    "\n",
    "$$\n",
    "A\n",
    "=\\sigma_1 u_1 v_1^T+\\sigma_2 u_2 v_2^T+\\cdots+\\sigma_r u_r v_r^T\n",
    "=\\sum_{j=1}^r \\sigma_j\\,u_j v_j^T.\n",
    "$$\n",
    "\n",
    "Each term $\\sigma_j u_j v_j^T$ is a **latent component** of $A$.\n",
    "Since $u_j v_j^T$ has linearly dependent columns, every term has rank $1$.\n",
    "\n",
    "### Proof\n",
    "\n",
    "Assume $m<n$. Then\n",
    "\n",
    "$$\n",
    "A=U\\Sigma V^T\n",
    "=[u_1\\;u_2\\;\\cdots\\;u_m]\n",
    "\\begin{pmatrix}\n",
    "\\sigma_1&&&\\\\\n",
    "&\\sigma_2&&\\\\\n",
    "&&\\ddots&\\\\\n",
    "&&&\\sigma_m\\\\\n",
    "&&&\\\\\n",
    "\\end{pmatrix}\n",
    "[v_1\\;v_2\\;\\cdots\\;v_n]^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "[\\sigma_1u_1\\;\\sigma_2u_2\\;\\cdots\\;\\sigma_m u_m]\n",
    "[v_1\\;v_2\\;\\cdots\\;v_n]^T\n",
    "=\\sum_{j=1}^m \\sigma_j u_j v_j^T.\n",
    "$$\n",
    "\n",
    "Since $\\sigma_j=0$ for $j>r$, this reduces to\n",
    "\n",
    "$$\n",
    "A=\\sum_{j=1}^r \\sigma_j u_j v_j^T.\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "1&0&-2\\\\\n",
    "0&1&-1\\\\\n",
    "2&1&1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "An SVD is\n",
    "\n",
    "$$\n",
    "U=\n",
    "\\begin{pmatrix}\n",
    "\\frac23&\\frac1{\\sqrt6}&-\\frac15\\\\\n",
    "\\frac16&\\frac1{\\sqrt{30}}&\\frac15\\\\\n",
    "\\frac16&-\\frac5{\\sqrt6}&0\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\Sigma=\n",
    "\\begin{pmatrix}\n",
    "6&0&0\\\\\n",
    "0&2&0\\\\\n",
    "0&0&1\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "V=\n",
    "\\begin{pmatrix}\n",
    "0&\\frac52&-\\frac15\\\\\n",
    "0&\\frac25&\\frac15\\\\\n",
    "1&0&0\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\sigma_1u_1v_1^T=\n",
    "\\begin{pmatrix}\n",
    "0&0&0\\\\\n",
    "0&0&0\\\\\n",
    "2&1&1\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\sigma_2u_2v_2^T=\n",
    "\\begin{pmatrix}\n",
    "5&4&-2\\\\\n",
    "5&2&-1\\\\\n",
    "0&0&0\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\sigma_3u_3v_3^T=\n",
    "\\begin{pmatrix}\n",
    "1&-2&0\\\\\n",
    "-2&4&0\\\\\n",
    "0&0&0\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Adding them gives\n",
    "\n",
    "$$\n",
    "A=\\sum_{j=1}^3 \\sigma_j u_j v_j^T.\n",
    "$$\n",
    "\n",
    "\n",
    "### Extra: SVD of the linear map\n",
    "\n",
    "For any vector $x$,\n",
    "\n",
    "$$\n",
    "L_A(x)=Ax=U\\Sigma V^Tx\n",
    "=U\n",
    "\\begin{pmatrix}\n",
    "\\sigma_1(v_1\\cdot x)\\\\\n",
    "\\sigma_2(v_2\\cdot x)\\\\\n",
    "\\vdots\\\\\n",
    "\\sigma_r(v_r\\cdot x)\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "=\\sum_{j=1}^r \\sigma_j (v_j\\cdot x)u_j.\n",
    "$$\n",
    "\n",
    "Once the SVD is known, computing $Ax$ reduces to inner products with $v_j$ and weighted sums of $u_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19257629faffb6e8",
   "metadata": {},
   "source": [
    "## Truncated SVD\n",
    "\n",
    "The alternative form of the SVD is the most important source of the applications of this decomposition. The more latent components you add, the closer you get to the matrix. Each of these partial sums is known as a truncated singular value decomposition. For this reason, for every\n",
    "$k\\in\\{1,\\dots,r\\}$ we define:\n",
    "\n",
    "$$\n",
    "A_k=\\sum_{j=1}^k \\sigma_j\\,u_j v_j^T\n",
    "$$\n",
    "\n",
    "The important thing about all of this is that, for all\n",
    "$1\\le k\\le r$, among all the matrices of rank $k$,\n",
    "$A_k$ is the one that most resembles $A$. This is the main reason why SVD is used in real applications. You can interpret it as the SVD arranging $A$ into its “most important” and “least important” pieces. For this reason, the largest singular values describe the broad strokes of $A$, whilst the smallest singular values take care of the finer details.\n",
    "\n",
    "The best way to approximate a high-rank matrix by a low-rank one is by discarding the pieces of its singular value decomposition which have the smallest singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe56ddea7bc2a82",
   "metadata": {},
   "source": [
    "Let's compute the truncated SVD for the matrix from the previous section:\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "1&1&1&1\\\\\n",
    "2&2&-2&-2\\\\\n",
    "3&-3&3&-3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Its latent components are:\n",
    "\n",
    "$$\n",
    "\\sigma_1u_1v_1^T=\n",
    "\\begin{pmatrix}\n",
    "0&0&0\\\\\n",
    "0&0&0\\\\\n",
    "2&1&1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_2u_2v_2^T=\n",
    "\\begin{pmatrix}\n",
    "\\frac54&\\frac52&-2\\\\\n",
    "\\frac52&5&-1\\\\\n",
    "0&0&0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_3u_3v_3^T=\n",
    "\\begin{pmatrix}\n",
    "5&1&0\\\\\n",
    "-\\frac52&\\frac54&0\\\\\n",
    "0&0&0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then, the best approximations (of rank $1$, $2$ and $3$ respectively) for $A$ are:\n",
    "\n",
    "$$\n",
    "A_1=\n",
    "\\begin{pmatrix}\n",
    "0&0&0\\\\\n",
    "0&0&0\\\\\n",
    "2&1&1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_2=\n",
    "\\begin{pmatrix}\n",
    "\\frac54&\\frac52&-2\\\\\n",
    "\\frac52&5&-1\\\\\n",
    "2&1&1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_3=\n",
    "\\begin{pmatrix}\n",
    "1&0&-2\\\\\n",
    "0&1&-1\\\\\n",
    "2&1&1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5e5732d9334d8",
   "metadata": {},
   "source": [
    "## Image compression\n",
    "\n",
    "Truncated singular value decomposition often retains a stunningly large level of accuracy even when the values of\n",
    "$k$ are much smaller than $r$. This is because, in real-world matrices, only a minuscule proportion of singular values are large. As a result,\n",
    "$A_k$ serves as an accurate approximation of $A$.\n",
    "\n",
    "This is particularly useful for image compression. A black and white image can be represented as a matrix with values from $0$ to $255$, where $0$ is full black and $255$ equals white. As the numbers increase, lighter and lighter shades are obtained. Let's see truncated SVD in action with this cute panda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e813117ce3b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open(\"img/panda/panda_org.webp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7438456f949fb",
   "metadata": {},
   "source": [
    "This image corresponds to a $350\\times 634$ matrix $A$. Since every column is nearly unique, the rank of $A$ is $350$—the biggest possible. This implies that there are $350$ latent components. The first singular value is the largest, and the first latent component is the best rank-$1$ approximation to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620dd2f43a90ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"img/panda/panda_k1.webp\")\n",
    "img.thumbnail((634, 350))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fcf5c470ea627",
   "metadata": {},
   "source": [
    "Perhaps it is not a good approximation, but note that as it is rank $1$, every row is a multiple of any other one—and the same occurs with the columns. Now look at the approximation with $k=5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf38d3d36f2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"img/panda/panda_k5.webp\")\n",
    "img.thumbnail((634, 350))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86176f99445a3c62",
   "metadata": {},
   "source": [
    "It is already getting better with only $5$ singular values. But when $k=10,20,50$, the results are amazing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5213b43609e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"img/panda/panda_k10.webp\")\n",
    "img.thumbnail((634, 350))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810788b374763058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"img/panda/panda_k20.webp\")\n",
    "img.thumbnail((634, 350))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d373be7a6bf639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"img/panda/panda_k50.webp\")\n",
    "img.thumbnail((634, 350))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15004d0dd1d25bc6",
   "metadata": {},
   "source": [
    "Using $50$ singular values already gives an excellent result, and note that this is much less than $350$. Since the next singular values are negligible, when $k=100,200$ the approximation is so good that the difference can no longer be distinguished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf9e84ab517066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"img/panda/panda_k100.webp\")\n",
    "img.thumbnail((634, 350))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7674b09d1cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"img/panda/panda_k200.webp\")\n",
    "img.thumbnail((634, 350))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a19e98472ce0cc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "When every singular value of $A$ is positive, the matrix is invertible and\n",
    "\n",
    "$$\n",
    "A^{-1}=V\\Sigma^{-1}U^T.\n",
    "$$\n",
    "\n",
    "The geometry of $A^{-1}$ and $A^T$ is closely related to that of $A$.\n",
    "\n",
    "The four fundamental spaces of $A$ are\n",
    "\n",
    "$$\n",
    "\\ker(L_A),\\quad \\operatorname{Im}(L_A),\\quad \\ker(L_{A^T}),\\quad \\operatorname{Im}(L_{A^T}),\n",
    "$$\n",
    "\n",
    "and the SVD of $A$ gives an orthonormal basis for each of them.\n",
    "\n",
    "The alternative form of the SVD of $A$ is the sum of its latent components:\n",
    "\n",
    "$$\n",
    "A=\\sum_{j=1}^r \\sigma_j\\,u_j v_j^T.\n",
    "$$\n",
    "\n",
    "The best way to approximate an $r$-rank matrix $A$ by a $k$-rank one ($k\\le r$) is its truncated SVD:\n",
    "\n",
    "$$\n",
    "A_k=\\sum_{j=1}^k \\sigma_j\\,u_j v_j^T.\n",
    "$$\n",
    "\n",
    "The singular values are ordered non-increasingly:\n",
    "\n",
    "$$\n",
    "\\sigma_1\\ge \\sigma_2\\ge \\cdots\\ge \\sigma_r>0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f5e113cd37275",
   "metadata": {},
   "source": [
    "## SVD in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9137e7cbc6d7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matrix from the image\n",
    "A = np.array([\n",
    "    [1,  1,  1,  1],\n",
    "    [2,  2, -2, -2],\n",
    "    [3, -3,  3, -3]\n",
    "], dtype=float)\n",
    "\n",
    "# SVD: A = U @ np.diag(s) @ Vt\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "print(\"A shape:\", A.shape)\n",
    "print(\"U shape:\", U.shape)\n",
    "print(\"s (singular values):\", s)\n",
    "print(\"Vt shape:\", Vt.shape)\n",
    "\n",
    "# Build Sigma with same shape as A (m x n)\n",
    "m, n = A.shape\n",
    "Sigma = np.zeros((m, n))\n",
    "Sigma[:len(s), :len(s)] = np.diag(s)\n",
    "\n",
    "# Verify reconstruction\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "print(\"\\nReconstruction error (Frobenius norm):\", np.linalg.norm(A - A_reconstructed))\n",
    "\n",
    "# Optional: rank (numerical)\n",
    "tol = 1e-12\n",
    "rank = np.sum(s > tol)\n",
    "print(\"Numerical rank:\", rank)\n",
    "\n",
    "# Optional: truncated SVD approximation (choose k)\n",
    "k = 2\n",
    "A_k = (U[:, :k] * s[:k]) @ Vt[:k, :]\n",
    "print(f\"\\nTruncated rank-{k} approximation error:\", np.linalg.norm(A - A_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8a98dbcf59d68",
   "metadata": {},
   "source": [
    "## SymPy can compute an exact (symbolic) SVD for that matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadb351cef5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "A = sp.Matrix([\n",
    "    [1, 0, -2],\n",
    "    [0, 1, -1],\n",
    "    [2, 1,  1]\n",
    "])\n",
    "\n",
    "A.singular_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5990d43ab8daf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = A.singular_value_decomposition()\n",
    "\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c27573231039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(U * S * V.T - A).simplify()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
