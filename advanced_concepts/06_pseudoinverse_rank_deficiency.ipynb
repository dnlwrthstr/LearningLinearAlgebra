{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499e7f3d",
   "metadata": {},
   "source": [
    "\n",
    "# Pseudoinverse and Rank-Deficient Systems\n",
    "\n",
    "This notebook introduces the **Moore–Penrose pseudoinverse** and explains how it resolves\n",
    "least squares problems when matrices are **non-square** or **rank-deficient**.\n",
    "\n",
    "The pseudoinverse is a unifying tool for:\n",
    "- Least squares\n",
    "- Minimum-norm solutions\n",
    "- Solving singular systems\n",
    "- Stable numerical computation via SVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18386384",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Preliminaries\n",
    "\n",
    "We assume familiarity with:\n",
    "\n",
    "- Least squares and projections\n",
    "- Matrix decompositions (especially SVD)\n",
    "- Rank and null space\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c72dc9",
   "metadata": {},
   "source": [
    "\n",
    "## Why the Inverse Is Not Enough\n",
    "\n",
    "The standard inverse $A^{-1}$ exists only if:\n",
    "- $A$ is square ($m=n$)\n",
    "- $A$ is full rank\n",
    "\n",
    "In many real problems:\n",
    "- $m \\neq n$ (rectangular matrices)\n",
    "- $\\operatorname{rank}(A) < \\min(m,n)$ (rank-deficient)\n",
    "\n",
    "We need a generalized inverse: the **pseudoinverse**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a655a97",
   "metadata": {},
   "source": [
    "\n",
    "## Moore–Penrose Pseudoinverse\n",
    "\n",
    "The **Moore–Penrose pseudoinverse** of $A$ is denoted $A^+$. It is the unique matrix satisfying:\n",
    "\n",
    "1. $$AA^+A = A$$\n",
    "2. $$A^+AA^+ = A^+$$\n",
    "3. $$(AA^+)^T = AA^+$$\n",
    "4. $$(A^+A)^T = A^+A$$\n",
    "\n",
    "These conditions define the canonical generalized inverse used in least squares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea43b5",
   "metadata": {},
   "source": [
    "\n",
    "## Pseudoinverse via SVD\n",
    "\n",
    "Let the SVD of $A$ be\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ contains singular values $\\sigma_1 \\ge \\cdots \\ge \\sigma_r > 0$ (rank $r$).\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "A^+ = V \\Sigma^+ U^T\n",
    "$$\n",
    "\n",
    "where $\\Sigma^+$ is formed by reciprocals of nonzero singular values.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a954d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T13:32:14.658419Z",
     "start_time": "2025-12-26T13:32:14.645490Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1., 2.],\n",
    "              [2., 4.],\n",
    "              [3., 6.]])  # rank-deficient (columns dependent)\n",
    "\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# Build Sigma^+\n",
    "tol = 1e-12\n",
    "s_inv = np.array([1/si if si > tol else 0 for si in s])\n",
    "Sigma_pinv = np.diag(s_inv)\n",
    "\n",
    "A_pinv = Vt.T @ Sigma_pinv @ U.T\n",
    "A_pinv\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01428571, 0.02857143, 0.04285714],\n",
       "       [0.02857143, 0.05714286, 0.08571429]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "73a36c99",
   "metadata": {},
   "source": [
    "\n",
    "## Least Squares with the Pseudoinverse\n",
    "\n",
    "The least squares solution minimizing $\\|Ax - b\\|$ is:\n",
    "\n",
    "$$\n",
    "x^* = A^+ b\n",
    "$$\n",
    "\n",
    "If $A$ has full column rank, this equals the familiar formula:\n",
    "\n",
    "$$\n",
    "x^* = (A^T A)^{-1} A^T b\n",
    "$$\n",
    "\n",
    "But $A^+ b$ remains valid when $A$ is rectangular or rank-deficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = np.array([1., 2., 2.])\n",
    "\n",
    "x_star = A_pinv @ b\n",
    "x_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03dd50",
   "metadata": {},
   "source": [
    "\n",
    "## Minimum-Norm Solutions (Underdetermined Systems)\n",
    "\n",
    "If $Ax=b$ has infinitely many solutions (typically when $m<n$ or rank-deficient),\n",
    "the pseudoinverse returns the **minimum Euclidean norm** solution:\n",
    "\n",
    "$$\n",
    "x^* = \\arg\\min_{Ax=b} \\|x\\|\n",
    "$$\n",
    "\n",
    "This is important in:\n",
    "- Regularization\n",
    "- Compressed sensing\n",
    "- Machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Underdetermined example: m < n\n",
    "A2 = np.array([[1., 0., 1.],\n",
    "               [0., 1., 1.]])\n",
    "b2 = np.array([1., 1.])\n",
    "\n",
    "A2_pinv = np.linalg.pinv(A2)\n",
    "x2 = A2_pinv @ b2\n",
    "\n",
    "x2, np.linalg.norm(x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3ec23",
   "metadata": {},
   "source": [
    "\n",
    "## Geometry: Projections Revisited\n",
    "\n",
    "Least squares can be summarized geometrically:\n",
    "\n",
    "- $Ax$ lies in the column space $\\mathcal{C}(A)$\n",
    "- $A^+ b$ produces an $x$ whose image $Ax$ is the orthogonal projection of $b$ onto $\\mathcal{C}(A)$\n",
    "- The residual is orthogonal to $\\mathcal{C}(A)$\n",
    "\n",
    "Pseudoinverse provides the algebraic mechanism behind this projection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05ded3",
   "metadata": {},
   "source": [
    "\n",
    "## Failure Modes and Practical Notes\n",
    "\n",
    "- Very small singular values cause large entries in $A^+$ (noise amplification)\n",
    "- Use a tolerance or truncated SVD for numerical robustness\n",
    "- In practice, use `np.linalg.pinv(A)` which handles tolerances safely\n",
    "\n",
    "Trade-off:\n",
    "> Accuracy vs. stability when singular values are near zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67ce2f",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- $A^+$ generalizes $A^{-1}$ to all matrices\n",
    "- Defined by Moore–Penrose conditions\n",
    "- Computed robustly via SVD\n",
    "- Gives least squares solution $x^* = A^+ b$\n",
    "- Produces minimum-norm solutions when solutions are not unique\n",
    "\n",
    "Next: **Canonical forms**, which classify linear operators up to change of basis.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
