{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb29490",
   "metadata": {},
   "source": [
    "\n",
    "# Least Squares and Optimization\n",
    "\n",
    "This notebook studies **least squares problems**, which arise when linear systems are\n",
    "overdetermined and exact solutions do not exist.\n",
    "\n",
    "Least squares provides the geometric and optimization-based foundation for data fitting,\n",
    "regression, signal processing, and machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b7bf3",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Preliminaries\n",
    "\n",
    "We assume familiarity with:\n",
    "\n",
    "- Inner product spaces\n",
    "- Orthogonal projections\n",
    "- Matrix decompositions (QR)\n",
    "\n",
    "Throughout, let:\n",
    "- $A \\in \\mathbb{R}^{m \\times n}$ with $m > n$\n",
    "- $b \\in \\mathbb{R}^m$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25c177",
   "metadata": {},
   "source": [
    "\n",
    "## Overdetermined Linear Systems\n",
    "\n",
    "Consider the system\n",
    "\n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "\n",
    "When $m > n$, an exact solution typically does not exist.\n",
    "Instead, we seek $x$ minimizing the residual:\n",
    "\n",
    "$$\n",
    "\\|Ax - b\\|\n",
    "$$\n",
    "\n",
    "This leads to the **least squares problem**:\n",
    "$$\n",
    "x^* = \\arg\\min_x \\|Ax - b\\|^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a25c87",
   "metadata": {},
   "source": [
    "\n",
    "## Geometric Interpretation\n",
    "\n",
    "Let $\\mathcal{C}(A)$ denote the column space of $A$.\n",
    "\n",
    "- $Ax$ lies in $\\mathcal{C}(A)$\n",
    "- We seek the vector in $\\mathcal{C}(A)$ closest to $b$\n",
    "- The residual $r = b - Ax^*$ is orthogonal to $\\mathcal{C}(A)$\n",
    "\n",
    "Thus, least squares corresponds to **orthogonal projection**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66a566",
   "metadata": {},
   "source": [
    "\n",
    "## Normal Equations\n",
    "\n",
    "The optimal solution satisfies:\n",
    "\n",
    "$$\n",
    "A^T (Ax - b) = 0\n",
    "$$\n",
    "\n",
    "which leads to the **normal equations**:\n",
    "\n",
    "$$\n",
    "A^T A x = A^T b\n",
    "$$\n",
    "\n",
    "If $A$ has full column rank, then $A^T A$ is invertible and\n",
    "\n",
    "$$\n",
    "x^* = (A^T A)^{-1} A^T b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c1a80",
   "metadata": {},
   "source": [
    "\n",
    "## Limitations of Normal Equations\n",
    "\n",
    "Although theoretically sound, normal equations have drawbacks:\n",
    "\n",
    "- $A^T A$ squares the condition number\n",
    "- Numerical instability for ill-conditioned $A$\n",
    "- Explicit matrix inversion is discouraged\n",
    "\n",
    "In practice, QR or SVD-based methods are preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33aa0a9",
   "metadata": {},
   "source": [
    "\n",
    "## Least Squares via QR Decomposition\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "A = QR\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q$ has orthonormal columns\n",
    "- $R$ is upper triangular\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\|Ax - b\\| = \\|Rx - Q^T b\\|\n",
    "$$\n",
    "\n",
    "Solve instead:\n",
    "\n",
    "$$\n",
    "Rx = Q^T b\n",
    "$$\n",
    "\n",
    "This approach is numerically stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b1a8e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T13:26:58.090794Z",
     "start_time": "2025-12-26T13:26:58.065391Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Overdetermined system\n",
    "A = np.array([[1., 1.],\n",
    "              [1., 2.],\n",
    "              [1., 3.]])\n",
    "b = np.array([1., 2., 2.])\n",
    "\n",
    "Q, R = np.linalg.qr(A)\n",
    "x_ls = np.linalg.solve(R, Q.T @ b)\n",
    "\n",
    "x_ls\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.5       ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6ba756c8",
   "metadata": {},
   "source": [
    "\n",
    "## Least Squares via SVD\n",
    "\n",
    "Using the SVD\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "the least squares solution is\n",
    "\n",
    "$$\n",
    "x^* = V \\Sigma^{-1} U^T b\n",
    "$$\n",
    "\n",
    "This method:\n",
    "- Handles rank-deficient systems\n",
    "- Produces minimum-norm solutions\n",
    "- Is the most robust numerically\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "569d94b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T13:27:20.798602Z",
     "start_time": "2025-12-26T13:27:20.732277Z"
    }
   },
   "source": [
    "\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "Sigma_inv = np.diag(1 / s)\n",
    "\n",
    "x_svd = Vt.T @ Sigma_inv @ U.T @ b\n",
    "x_svd\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.5       ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "be3ee619",
   "metadata": {},
   "source": [
    "\n",
    "## Connection to Optimization\n",
    "\n",
    "Least squares minimizes a convex quadratic function:\n",
    "\n",
    "$$\n",
    "f(x) = \\|Ax - b\\|^2\n",
    "$$\n",
    "\n",
    "- Gradient: $2A^T(Ax - b)$\n",
    "- Hessian: $2A^T A$\n",
    "\n",
    "This problem has:\n",
    "- A unique global minimum (if full rank)\n",
    "- No local minima other than the global one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d37bac",
   "metadata": {},
   "source": [
    "\n",
    "## Failure Modes and Practical Notes\n",
    "\n",
    "- Rank-deficient $A$ leads to non-unique solutions\n",
    "- Noise affects residuals but not formulation\n",
    "- QR is preferred for routine problems\n",
    "- SVD is preferred for ill-conditioned systems\n",
    "\n",
    "Least squares is robust but not foolproof.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34403f0",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Least squares solves inconsistent systems\n",
    "- Geometric meaning: orthogonal projection\n",
    "- Normal equations explain theory\n",
    "- QR and SVD provide stable computation\n",
    "- Central to data fitting and ML\n",
    "\n",
    "Next: **Pseudoinverse and rank-deficient systems**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
