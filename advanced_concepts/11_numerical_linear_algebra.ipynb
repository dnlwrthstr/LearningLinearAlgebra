{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037cbeb3",
   "metadata": {},
   "source": [
    "\n",
    "# Numerical Linear Algebra\n",
    "\n",
    "This notebook introduces core concepts in **numerical linear algebra**: how linear algebra behaves\n",
    "on real computers with finite precision.\n",
    "\n",
    "The central theme is:\n",
    "\n",
    "> Computers do not do exact arithmetic. Numerical linear algebra studies stability, error, and reliable algorithms.\n",
    "\n",
    "This topic is essential for scientific computing, optimization, and machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704d264",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Preliminaries\n",
    "\n",
    "We assume familiarity with:\n",
    "\n",
    "- Norms and inner products\n",
    "- Matrix decompositions (LU/QR/SVD)\n",
    "- Least squares\n",
    "\n",
    "We focus on conceptual understanding with practical examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc2dce",
   "metadata": {},
   "source": [
    "\n",
    "## Floating-Point Arithmetic\n",
    "\n",
    "Real numbers are stored approximately in floating-point form.\n",
    "\n",
    "Key consequence:\n",
    "- Basic operations introduce **rounding error**\n",
    "- Errors can accumulate and amplify\n",
    "\n",
    "Machine epsilon $\\varepsilon$ (for double precision) is approximately:\n",
    "\n",
    "$$\n",
    "\\varepsilon \\approx 2^{-52} \\approx 2.22 \\times 10^{-16}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "282c3816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:29:33.241033Z",
     "start_time": "2025-12-26T14:29:33.226172Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "np.finfo(float).eps\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.220446049250313e-16)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ace5811e",
   "metadata": {},
   "source": [
    "\n",
    "## Conditioning vs. Stability\n",
    "\n",
    "Two separate ideas:\n",
    "\n",
    "### Conditioning (problem property)\n",
    "How sensitive is the *true solution* to small perturbations in the input?\n",
    "\n",
    "### Stability (algorithm property)\n",
    "Does the algorithm produce a solution close to the true solution of a nearby problem?\n",
    "\n",
    "A well-conditioned problem can be solved poorly by an unstable algorithm,\n",
    "and a well-designed stable algorithm can handle moderately ill-conditioned problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee36686",
   "metadata": {},
   "source": [
    "\n",
    "## Condition Number\n",
    "\n",
    "For an invertible matrix $A$, the condition number (in a consistent norm) is:\n",
    "\n",
    "$$\n",
    "\\kappa(A) = \\|A\\|\\,\\|A^{-1}\\|\n",
    "$$\n",
    "\n",
    "For the 2-norm:\n",
    "\n",
    "$$\n",
    "\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\n",
    "$$\n",
    "\n",
    "Large $\\kappa(A)$ means:\n",
    "- small input errors can cause large output errors\n",
    "- numerical results may be unreliable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = np.array([[1., 1.],\n",
    "              [1., 1.000000000001]])\n",
    "\n",
    "# 2-norm condition number via SVD\n",
    "np.linalg.cond(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0aa94",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Error and Backward Error\n",
    "\n",
    "Let $\\hat{x}$ be the computed solution to $Ax=b$ and $x$ be the true solution.\n",
    "\n",
    "### Forward error\n",
    "$$\n",
    "\\frac{\\|x - \\hat{x}\\|}{\\|x\\|}\n",
    "$$\n",
    "\n",
    "### Backward error\n",
    "Smallest perturbation $\\Delta A$, $\\Delta b$ such that\n",
    "$$\n",
    "(A + \\Delta A)\\hat{x} = b + \\Delta b\n",
    "$$\n",
    "\n",
    "Stable algorithms typically have small backward error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4156e6",
   "metadata": {},
   "source": [
    "\n",
    "## Why Normal Equations Are Risky\n",
    "\n",
    "For least squares, normal equations use:\n",
    "\n",
    "$$\n",
    "A^T A x = A^T b\n",
    "$$\n",
    "\n",
    "But conditioning worsens:\n",
    "\n",
    "$$\n",
    "\\kappa(A^T A) \\approx \\kappa(A)^2\n",
    "$$\n",
    "\n",
    "This can destroy accuracy for ill-conditioned problems.\n",
    "\n",
    "Preferred methods:\n",
    "- QR decomposition\n",
    "- SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1cfeb3bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:32:23.492629Z",
     "start_time": "2025-12-26T14:32:23.430096Z"
    }
   },
   "source": [
    "\n",
    "# Demonstrate condition number squaring effect\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "A = rng.standard_normal((50, 5))\n",
    "cond_A = np.linalg.cond(A)\n",
    "cond_AtA = np.linalg.cond(A.T @ A)\n",
    "\n",
    "cond_A, cond_AtA\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.6674837750724045), np.float64(2.780502140129717))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "bd93efde",
   "metadata": {},
   "source": [
    "\n",
    "## Stable Solves: LU with Pivoting, QR, and SVD\n",
    "\n",
    "Common guidance:\n",
    "\n",
    "- Use LU with partial pivoting for square systems\n",
    "- Use QR for least squares\n",
    "- Use SVD for rank-deficient or ill-conditioned problems\n",
    "\n",
    "Orthogonal transformations (QR/SVD) are stable because they preserve norms.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d389cefa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:33:10.247836Z",
     "start_time": "2025-12-26T14:33:10.189112Z"
    }
   },
   "source": [
    "\n",
    "# Compare least squares solutions: normal equations vs QR on a mildly ill-conditioned problem\n",
    "rng = np.random.default_rng(1)\n",
    "m, n = 80, 10\n",
    "\n",
    "A = rng.standard_normal((m, n))\n",
    "# Make A more ill-conditioned by scaling columns\n",
    "scales = np.logspace(0, 8, n)\n",
    "A = A * scales\n",
    "\n",
    "x_true = rng.standard_normal(n)\n",
    "b = A @ x_true + 1e-6 * rng.standard_normal(m)\n",
    "\n",
    "# Normal equations\n",
    "x_ne = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "\n",
    "# QR\n",
    "Q, R = np.linalg.qr(A)\n",
    "x_qr = np.linalg.solve(R, Q.T @ b)\n",
    "\n",
    "# SVD\n",
    "x_svd = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "err_ne = np.linalg.norm(x_ne - x_true) / np.linalg.norm(x_true)\n",
    "err_qr = np.linalg.norm(x_qr - x_true) / np.linalg.norm(x_true)\n",
    "err_svd = np.linalg.norm(x_svd - x_true) / np.linalg.norm(x_true)\n",
    "\n",
    "err_ne, err_qr, err_svd\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(4.907474066468839e-07),\n",
       " np.float64(5.6659893189839414e-08),\n",
       " np.float64(5.583683722433508e-08))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ead06266",
   "metadata": {},
   "source": [
    "\n",
    "## Iterative Methods (High Level)\n",
    "\n",
    "For very large problems, direct factorization may be too expensive.\n",
    "Iterative methods construct approximations:\n",
    "\n",
    "- Gradient descent (for least squares / SPD systems)\n",
    "- Conjugate Gradient (SPD systems)\n",
    "- GMRES (general systems)\n",
    "\n",
    "Many are based on **Krylov subspaces**:\n",
    "$$\n",
    "\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, A^2 r_0, \\dots, A^{k-1}r_0\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d59169",
   "metadata": {},
   "source": [
    "\n",
    "## Practical Notes\n",
    "\n",
    "- Always examine conditioning before trusting results\n",
    "- Prefer orthogonal-factor methods (QR/SVD) for stability\n",
    "- Avoid explicit matrix inverses\n",
    "- Use tolerances and regularization when singular values are tiny\n",
    "\n",
    "Numerical linear algebra is about **reliable computation**, not just formulas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599eb1be",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Floating-point arithmetic introduces rounding error\n",
    "- Conditioning measures problem sensitivity\n",
    "- Stability measures algorithm reliability\n",
    "- Condition number controls attainable accuracy\n",
    "- Normal equations can be numerically dangerous\n",
    "- QR and SVD are stable workhorses\n",
    "- Iterative methods scale to large problems\n",
    "\n",
    "Next: **Linear algebra in machine learning**, synthesizing these tools in applied settings.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
