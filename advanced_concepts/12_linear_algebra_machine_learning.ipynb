{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c126ef6d",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Algebra in Machine Learning\n",
    "\n",
    "This notebook synthesizes the previous topics and shows how **linear algebra underpins modern\n",
    "machine learning**.\n",
    "\n",
    "Rather than introducing new mathematics, the focus here is on:\n",
    "- How core linear algebra concepts appear in ML pipelines\n",
    "- Why numerical stability and geometry matter\n",
    "- How decompositions, projections, and spectra drive learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ee552",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Preliminaries\n",
    "\n",
    "We assume familiarity with:\n",
    "\n",
    "- Least squares and optimization\n",
    "- Matrix decompositions (QR, SVD)\n",
    "- Inner products and orthogonality\n",
    "- Numerical linear algebra concepts\n",
    "\n",
    "All examples are linear or locally linear approximations of ML models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a112",
   "metadata": {},
   "source": [
    "\n",
    "## Data as Matrices\n",
    "\n",
    "In supervised learning, data is typically represented as:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ = number of samples\n",
    "- $d$ = number of features\n",
    "\n",
    "Targets:\n",
    "$$\n",
    "y \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "Many learning problems reduce to solving or approximating:\n",
    "$$\n",
    "Xw \\approx y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb431fb4",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Regression Revisited\n",
    "\n",
    "Linear regression solves:\n",
    "\n",
    "$$\n",
    "\\min_w \\|Xw - y\\|^2\n",
    "$$\n",
    "\n",
    "Solution:\n",
    "$$\n",
    "w^* = X^+ y\n",
    "$$\n",
    "\n",
    "This connects directly to:\n",
    "- Least squares\n",
    "- Pseudoinverse\n",
    "- Projections onto $\\mathcal{C}(X)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5355a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:36:47.485831Z",
     "start_time": "2025-12-26T14:36:47.448998Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic regression data\n",
    "rng = np.random.default_rng(0)\n",
    "n, d = 100, 5\n",
    "\n",
    "X = rng.standard_normal((n, d))\n",
    "w_true = rng.standard_normal(d)\n",
    "y = X @ w_true + 0.1 * rng.standard_normal(n)\n",
    "\n",
    "w_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "np.linalg.norm(w_hat - w_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b71b81",
   "metadata": {},
   "source": [
    "\n",
    "## Covariance and PCA\n",
    "\n",
    "Given centered data $X$, the empirical covariance matrix is:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n} X^T X\n",
    "$$\n",
    "\n",
    "Principal Component Analysis (PCA):\n",
    "- Finds orthogonal directions of maximum variance\n",
    "- Corresponds to eigenvectors of $C$\n",
    "- Equivalent to SVD of $X$\n",
    "\n",
    "Low-rank approximation:\n",
    "$$\n",
    "X \\approx U_k \\Sigma_k V_k^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13e8ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:37:38.424555Z",
     "start_time": "2025-12-26T14:37:38.318280Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# PCA via SVD\n",
    "Xc = X - X.mean(axis=0)\n",
    "\n",
    "U, s, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "\n",
    "# Explained variance (up to scale)\n",
    "explained_variance = s**2 / np.sum(s**2)\n",
    "explained_variance[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc785b",
   "metadata": {},
   "source": [
    "\n",
    "## Geometry of Optimization\n",
    "\n",
    "Many ML objectives are sums of quadratic or locally quadratic terms.\n",
    "\n",
    "Near an optimum $w^*$:\n",
    "\n",
    "$$\n",
    "f(w) \\approx f(w^*) + \\frac{1}{2}(w - w^*)^T H (w - w^*)\n",
    "$$\n",
    "\n",
    "where $H$ is the Hessian.\n",
    "\n",
    "Eigenvalues of $H$ determine:\n",
    "- Conditioning of the problem\n",
    "- Convergence speed of gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dad3fa",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Descent and Spectra\n",
    "\n",
    "For quadratic loss with Hessian $H$:\n",
    "\n",
    "- Convergence rate depends on $\\kappa(H)$\n",
    "- Large eigenvalue spread slows learning\n",
    "- Preconditioning improves conditioning\n",
    "\n",
    "This links optimization directly to spectral theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530025e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:42:05.747314Z",
     "start_time": "2025-12-26T14:42:05.689150Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simple illustration: gradient descent on a quadratic\n",
    "H = np.diag([1., 10., 100.])  # ill-conditioned Hessian\n",
    "w = np.ones(3)\n",
    "eta = 0.01\n",
    "\n",
    "for _ in range(1000):\n",
    "    w = w - eta * (H @ w)\n",
    "\n",
    "np.linalg.norm(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40739b",
   "metadata": {},
   "source": [
    "\n",
    "## Regularization as Linear Algebra\n",
    "\n",
    "Ridge regression solves:\n",
    "\n",
    "$$\n",
    "\\min_w \\|Xw - y\\|^2 + \\lambda \\|w\\|^2\n",
    "$$\n",
    "\n",
    "Solution:\n",
    "$$\n",
    "w^* = (X^T X + \\lambda I)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "- Improves conditioning\n",
    "- Shrinks small singular directions\n",
    "- Controls variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537bd84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T14:42:45.185331Z",
     "start_time": "2025-12-26T14:42:45.130142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ridge regression effect\n",
    "lam = 1.0\n",
    "w_ridge = np.linalg.solve(X.T @ X + lam * np.eye(d), X.T @ y)\n",
    "\n",
    "np.linalg.norm(w_ridge), np.linalg.norm(w_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d0c1e",
   "metadata": {},
   "source": [
    "\n",
    "## Neural Networks (Linear Viewpoint)\n",
    "\n",
    "Even deep networks rely on linear algebra:\n",
    "\n",
    "- Layers are affine maps: $Wx + b$\n",
    "- Backpropagation uses Jacobians\n",
    "- Weight initialization affects spectral properties\n",
    "- Low-rank structure appears in trained weights\n",
    "\n",
    "Nonlinearity composes linear maps, but does not remove their importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ad5ba",
   "metadata": {},
   "source": [
    "\n",
    "## Numerical Stability in ML\n",
    "\n",
    "Key lessons from numerical linear algebra:\n",
    "\n",
    "- Avoid explicit inverses\n",
    "- Monitor conditioning\n",
    "- Use SVD/QR-based solvers\n",
    "- Normalize and center data\n",
    "- Regularize ill-posed problems\n",
    "\n",
    "Many ML failures are numerical, not statistical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd4daa",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- ML problems are linear algebra problems with noise\n",
    "- Least squares and pseudoinverses underlie regression\n",
    "- PCA is spectral analysis of covariance\n",
    "- Optimization speed depends on eigenvalues\n",
    "- Regularization improves conditioning\n",
    "- Numerical stability is essential for reliable learning\n",
    "\n",
    "This concludes the structured overview of **advanced linear algebra**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
